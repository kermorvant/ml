{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## A simple example on a toy problem\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Automatic text classification is one of the standard Machine Learning applications. Spam filtering was the first service based on Machine Learing used by almost every internet users.\n",
    "\n",
    "We want to build a email classification system using a Naive Bayes classifier.\n",
    "Emails must be classified into 2 different classes : *membership* or *complaint*.\n",
    "\n",
    " The classification will be based on the presence or absence of 3 words : \n",
    " \n",
    " `join, address,  unacceptable, `\n",
    " \n",
    "The training sample is composed of following documents, presented using a bag-of-word representation : \n",
    "\n",
    "\n",
    "join |  address | unacceptable  | class \n",
    "---|--------|-------|--------\n",
    "1|1|0| membership\n",
    "0|0|0| membership\n",
    "1|1|0| membership\n",
    "1|1|0| membership\n",
    "1|1|0| membership\n",
    "0|1|1| membership\n",
    "1|1|0| membership\n",
    "0|1|1| complaint\n",
    "1|1|0| complaint\n",
    "0|0|1| complaint\n",
    "0|1|1| complaint\n",
    "0|1|1| complaint\n",
    "1|0|0| complaint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We denote by  $Y$ the document class and  $X=X_1,\\cdots,X_3$ the bag of word features.\n",
    "\n",
    "**Question**:\n",
    "> * Compute the frequency tables for each event and each class\n",
    "> \n",
    "> Class |  \"join\" present | \"address\" present  | \"unacceptable\" present \n",
    "> ---|--------|-------|--------\n",
    "> membership | | |\n",
    "> complaint | | |\n",
    ">  \n",
    "> * Divide the values in the table by the frequency of each class to compute P(word i is present|Y)\n",
    "> * Compute the probability of P(word i is absent|Y) : this is 1-P(word i is present|Y)\n",
    "> * Compute the prior  probabilities P(Y=membership) and P(Y=complaint)\n",
    "\n",
    "\n",
    "We want to predict the class of the following document using a Naives Bayes classifier \n",
    "\n",
    "*Sir, I have a new problem with my account : I can not login. This is the third time this week that my account is down and I can join no one. This is unacceptable.*\n",
    "\n",
    "In this text : \"join\" is present, \"address\" is not present and \"unacceptable\" is present\n",
    "\n",
    "**Question**:\n",
    "    \n",
    ">Based on the probabilities computed before, compute the most probable class using the Bayes Formula :\n",
    ">\n",
    "> $$ P(Y | X_1,X_2,X_3) = \\frac{P(X_1 | Y)P(X_2 | Y)P(X_3 | Y)P(Y)}{P(X)}$$\n",
    "\n",
    "> To simplify the task, we have already computed  P(X) = 0.04203733\n",
    ">\n",
    "> You will have to compute  :\n",
    ">\n",
    "> * $P(Y = membership | X_1,X_2,X_3)$   = P(\"join\" is present | Y = membership) \\* P(\"address\" is absent | Y = membership ) \\* P(\"unacceptable\" is present| Y = membership) \\* P( Y = membership) / P(X)\n",
    "> * $P(Y = complaint | X_1,X_2,X_3)$   = P(\"join\" is present | Y = complaint) \\* P(\"address\" is absent | Y = complaint ) \\* P(\"unacceptable\" is present| Y = complaint) \\* P( Y = complaint) / P(X)\n",
    "> * and select the class with the maximal probability.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "The following DataFrame contains the data of the previous exercice. \n",
    "\n",
    "**Question**\n",
    "> * Train a [Bernouilli Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) on all the data.\n",
    "> * Predict the class of the sample `sample = np.array([1,0,1])`, corresponding to the document of the previous excercise. \n",
    "> * Check your prediction with `clf.predict_proba([1,0,1])`\n",
    "> * Check the different probabilities computed for the previous question :\n",
    ">  * `clf.feature_count_` :  the count for each feature per class $C(X_i =1 \\| Y=c)$\n",
    ">  * `clf.feature_log_prob_` : the log probability for each feature per class $log(P(X_i =1 \\| Y=c))$\n",
    ">  * `clf.class_log_prior_` : the log prior probabilities for each class $log(P(Y=c))$\n",
    "> * Check your prediction with `clf.predict_proba([1,0,1])` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame([\n",
    "    [1,1,0, 'membership'],\n",
    "[0,0,0, 'membership'],\n",
    "[1,1,0, 'membership'],\n",
    "[1,1,0, 'membership'],\n",
    "[1,1,0, 'membership'],\n",
    "[0,1,1, 'membership'],\n",
    "[1,1,0, 'membership'],\n",
    "[0,1,1, 'complaint'],\n",
    "[1,1,0, 'complaint'],\n",
    "[0,0,1, 'complaint'],\n",
    "[0,1,1, 'complaint'],\n",
    "[0,1,1, 'complaint'],\n",
    "[1,0,0, 'complaint'],\n",
    "    ],\n",
    "    columns=['join',  'address',  'unacceptable', 'class'],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that we have the same result with BernoulliNB from sklearn\n",
    "# Define the features and class\n",
    "X = df[['join',  'address',  'unacceptable']]\n",
    "y = df['class']\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = None # YOUR CODE HERE ; use alpha = 0 to disable smoothing to compare with the previous exercise\n",
    "clf.fit(None,None) # YOUR CODE HERE\n",
    "print (\"frequence per event and per class\")\n",
    "print (clf.feature_count_)\n",
    "print (\"proba per event and per class\")\n",
    "print (np.exp(clf.feature_log_prob_))\n",
    "print (\"class prior\")\n",
    "print (np.exp(clf.class_log_prior_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the vector for the text to be classified \n",
    "# \"join\" is present, address is absent and unacceptable is present -> [1,0,1]\n",
    "sample = np.array(None) # YOUR CODE HERE\n",
    "# predict the class\n",
    "print (clf.predict(sample.reshape(1, -1))) # reshape to avoid a warning\n",
    "print (clf.classes_)\n",
    "# predict the probability for each class\n",
    "print (clf.predict_proba(sample.reshape(1, -1))) # reshape to avoid a warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeMonde2003 Dataset\n",
    "\n",
    "We will now apply classification algorithms to newspaper articles published in 2003 in *Le Monde*. The dataset can be dowloaded in  CSV format [here](https://kermorvant.github.io/ml/data/LeMonde2003_9classes.csv.gz).\n",
    "\n",
    "These articles concern different subjects but we will consider only articles related to the following subjects : entreprises (ENT), international (INT), arts (ART), société (SOC), France (FRA), sports (SPO), livres (LIV), télévision (TEL) and the font page articles (UNE).\n",
    "\n",
    "\n",
    "> * Load the CSV file `LeMonde2003_9classes.csv.gz` containing the articles using [pd.read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) . \n",
    "> * Plot the frequency histogram of the categories using [`countplot`](https://seaborn.pydata.org/tutorial/categorical.html) : `sns.countplot(data=df,y='category')`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# load dataframe from CSV file\n",
    "df = pd.read_csv('LeMonde2003_9classes.csv.gz')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the statistics of category\n",
    "sns.countplot(data=df,y='None')# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print examples of the articles\n",
    "print (\"Category:\",df.iloc[100]['category'])\n",
    "print (df.iloc[100]['text'])\n",
    "print ()\n",
    "print (\"Category:\",df.iloc[10000]['category'])\n",
    "print (df.iloc[10000]['text'])\n",
    "print ()\n",
    "print (\"Category:\",df.iloc[5008]['category'])\n",
    "print (df.iloc[5008]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-word representation\n",
    "\n",
    "In order to apply machine learning algorithms to text, documents must be transformed into vectors. The most simple and standard way to transform a document into a vector is the *bag-of-word* encoding.\n",
    "\n",
    "The idea is very simple : \n",
    "\n",
    "1. define the set of all the possible words that can appear in a document; denote its size by `max_features`.\n",
    "2. for each document,  encode it with a vector of size `max_features`, with the value of the ith component of the vector equal to the number of time the ith word appears in the document.\n",
    "\n",
    "See [the wikipedia article on Bag-of-word](https://en.wikipedia.org/wiki/Bag-of-words_model) for an example.\n",
    "\n",
    "Scikit-learn proposes different methods to encode text into vectors : [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).\n",
    "\n",
    "The encoder must first be trained on the train set and applied to the different sets, for example with the 1000  words : \n",
    "\n",
    "\tfrom sklearn.feature_extraction.text import CountVectorizer\n",
    "\tvectorizer = CountVectorizer(max_features=1000)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train_counts = vectorizer.transform(X_train)\n",
    "    X_test_counts = vectorizer.transform(X_test)\n",
    "        \n",
    "**Question**:\n",
    "\n",
    "> * Split the dataset LeMonde2003 into train/dev/test set. \n",
    "> * For each set, transform the text of the articles into vectors using the `CountVectorizer` with `max_features=1000` words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset (X and y together)\n",
    "df_train_dev, df_test = train_test_split(None ,test_size=0.20) # YOUR CODE HERE\n",
    "df_train, df_dev = train_test_split(None ,test_size=0.25) # YOUR CODE HERE\n",
    "\n",
    "print ('train size',df_train.shape)\n",
    "print ('dev size', df_dev.shape)\n",
    "print ('test size', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create features X and target y\n",
    "X_train = df_train.text\n",
    "X_dev = df_dev.text\n",
    "X_test = df_test.text\n",
    "\n",
    "y_train = df_train.category\n",
    "y_dev = df_dev.category\n",
    "y_test = df_test.category\n",
    "\n",
    "X_train_dev = df_train_dev.text\n",
    "y_train_dev = df_train_dev.category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# create the vectorizer object\n",
    "vectorizer = CountVectorizer(max_features=None) # YOUR CODE HERE\n",
    "# fit on train data\n",
    "vectorizer.fit(None) # YOUR CODE HERE\n",
    "# apply it on train and dev data\n",
    "X_train_counts = vectorizer.transform(None) # YOUR CODE HERE\n",
    "X_dev_counts = vectorizer.transform(None) # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print one sample text\n",
    "print (X_train.iloc[10000])\n",
    "# and its Count (sparse) representation \n",
    "print (X_train_counts[10000])\n",
    "# Inverse vocabulary \n",
    "terms = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "indices = np.array(list(vectorizer.vocabulary_.values()))\n",
    "inverse_vocabulary = terms[np.argsort(indices)]\n",
    "most_frequent = X_train_counts[10000].argmax()\n",
    "print (\"most frequent word =\",inverse_vocabulary[most_frequent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a mutlinomial Naive Bayes\n",
    "clf = None # YOUR CODE HERE\n",
    "# Train \n",
    "clf.fit(None,None) # YOUR CODE HERE\n",
    "# Evaluate \n",
    "print (clf.score(None,None)) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF representation\n",
    "\n",
    "The `CountVectorizer` encodes the text using the raw frequencies of the words. However, words that are very frequent and appear in all the documents will have a strong weight whereas they are not discriminative. The *Term-Frequency Inverse-Document-Frequency* weighting scheme take into accound the number of documents in which a given word occurs. A word that appear in many document will have less weight. See [the wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for more details.\n",
    "\n",
    "With scikit-learn, the `TfidfTransformer` is applied after the `CountVectorizer` :\n",
    "\n",
    "\tfrom sklearn.feature_extraction.text import TfidfTransformer\n",
    "\ttf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    " \tX_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\tX_test_tf = tf_transformer.transform(X_test_counts)\n",
    "\t\n",
    "**Question**:\n",
    "\n",
    "> * Use the TF-IDF representation to train a Multinomial Naive Bayes classifier. Report your best test error rate and the error rates for all the configurations tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "alpha = 0.001\n",
    "max_words =10000\n",
    "max_df = 1.0\n",
    "min_df = 0.0\n",
    "vectorizer = CountVectorizer(max_features=max_words,max_df=max_df,min_df=min_df)\n",
    "vectorizer.fit(X_train)\n",
    "X_train_counts = vectorizer.transform(X_train)\n",
    "X_dev_counts = vectorizer.transform(X_dev)\n",
    "\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_dev_tf = tf_transformer.transform(X_dev_counts)\n",
    "                 \n",
    "clf = MultinomialNB(alpha=alpha)\n",
    "clf.fit(None, None) # YOUR CODE HERE\n",
    "predict_train = clf.predict(None)# YOUR CODE HERE\n",
    "print (\"Train score\",accuracy_score(None,None))# YOUR CODE HERE\n",
    "predict_dev = clf.predict(None)# YOUR CODE HERE\n",
    "print (\"Dev score\",accuracy_score(None,None))# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "The classification error rate give an evaluation of the performance for all the classes. But since the classes are not equally distributed, they may not be equally well modelized. In order to get a better idea of the performance of the classifier, detailed metrics must be used : \n",
    "\n",
    "* [metrics.classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) provides a detailed analysis per class : the precision (amongst all the example classified as class X, how many are really from the classX) and the recall (amongst all the example that are from the class X, how many are classified as class X) and the F-Score which is as a weighted harmonic mean of the precision and recall.\n",
    "* [metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) which give the confusions between the classes.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "> * Report the `classification_report` for your  classifier. Which classes have the best scores ? Why ?\n",
    "> * Report the `confusion_matrix` for your  classifier. Which classes are the most confused ? Why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(None,None)) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(None,None) # YOUR CODE HERE\n",
    "print (conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Better display of the confusion matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "  \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "plot_confusion_matrix(conf_mat,classes=sorted(y_train.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "The classification process has many parameters : alpha for the classifier, max_words, max_df, min_df, ngram orders for the Count of TfIDF transformer. These parameters can be optimized by a grid search using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperameters optimization with GridSearchCV = parallel processing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.0001, 0.001,0.01,0.1)\n",
    "}\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=2)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train_dev, y_train_dev)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
