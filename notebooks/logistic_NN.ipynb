{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "In this notebook, you will program a complete classifier based on logistic regression seen as a neural network. The different steps are quite similar across the  all the kinds of neural networks but with different complexity. We will start with the simpliest one.\n",
    "\n",
    "## Logistic regresssion as a neural network\n",
    "\n",
    "The logistic regression classifier can be seen as a simple neural network : \n",
    "\n",
    "<img src=\"https://kermorvant.github.io/ml/images/logistic_NN.png\" style=\"width:650px\" >\n",
    "\n",
    "The weights are represented by the matrix W, the intercept by the bias b and the activation function is a sigmoid.\n",
    "\n",
    "### Basic functions\n",
    "\n",
    "First, we need to define 2 basic functions\n",
    "* the sigmoid activation functions. \n",
    "* the initialization function for the W and b parameters. \n",
    "\n",
    "In this notebook, the skeleton of the functions are given, you have to fill the spaces where there is a comment #YOUR CODE HERE. To check that your code is correct, a test is provided after each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sigmoid function for activation\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    \n",
    "    :param z: a scalar or numpy array \n",
    "    :returns: sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-z)) #YOUR CODE HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "# Check that your code is correct on one example\n",
    "test_result = sigmoid(np.array([0,2]))\n",
    "assert_allclose(test_result,[0.5,0.880797])\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_initialize(dim):\n",
    "    \"\"\"\n",
    "    Initialize w and b to 0\n",
    "\n",
    "    :param dim: size of the vector w    \n",
    "    :returs: w vector of zeros of size (dim, 1) and b = 0\n",
    "\n",
    "    \"\"\"\n",
    "    w = np.zeros((dim,1)) #YOUR CODE HERE\n",
    "    b = 0 #YOUR CODE HERE\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "# Check that your code is correct on one example\n",
    "dim = 2\n",
    "w, b = zero_initialize(dim)\n",
    "assert(w.shape == (dim, 1))\n",
    "assert(isinstance(b, float) or isinstance(b, int))\n",
    "assert (b==0)\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward step\n",
    "\n",
    "The forward step consist in computing the prediction of the network. This step is used both during the training and , when the training is done, when the netwok is used to make predictions. In the next function, we will compute both the output of the network and the gradient to update the parameters.\n",
    "\n",
    "The activation is \n",
    "\\begin{equation*}\n",
    "A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\n",
    "\\end{equation*}\n",
    "The loss function on a sample i is : \n",
    "\\begin{equation*}\n",
    "L(\\hat a^{(i)}, y^{(i)}) = - y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "\\end{equation*}\n",
    "\n",
    "The cost computed on m samples is \n",
    "\\begin{equation*}\n",
    "J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "\\end{equation*}\n",
    "\n",
    "The gradient for each parameter is : \n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Compute the forward propagation and compute cost and gradient\n",
    "\n",
    "    \n",
    "    :param w: weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    :param b: bias, a scalar\n",
    "    :param X: data of size (num_px * num_px * 3, number of examples)\n",
    "    :param Y: the target vector of size (1, number of examples)\n",
    "\n",
    "    :returns:\n",
    "    cost: negative log-likelihood cost for logistic regression\n",
    "    dw: gradient of the loss with respect to w, thus same shape as w\n",
    "    db: gradient of the loss with respect to b, thus same shape as b\n",
    " \n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    #print(m)\n",
    "    # Compute activation\n",
    "    A = sigmoid((w.T).dot(X)+b) #YOUR CODE HERE\n",
    "    #print (A)\n",
    "    # Compute the cost\n",
    "    c1 = np.multiply(Y,np.log(A))+ np.multiply((1-Y),(np.log(1-A)))\n",
    "    #print (c1)\n",
    "    cost = -1.0/m*np.sum(c1) #YOUR CODE HERE\n",
    "    #print (cost)\n",
    "    # Compute derivative for gradient computation\n",
    "    dw = 1/m*X.dot((A-Y).T) #YOUR CODE HERE\n",
    "    db = 1/m*np.sum(A-Y) #YOUR CODE HERE\n",
    "\n",
    "    grads = {\"dw\": dw,\"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "# Check that your code is correct on one example\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = forward(w, b, X, Y)\n",
    "assert(grads[\"dw\"].shape == w.shape)\n",
    "assert(grads[\"db\"].dtype == float)\n",
    "cost = np.squeeze(cost)\n",
    "assert(cost.shape == ())\n",
    "assert_allclose(grads[\"dw\"],[[0.99845601],[2.39507239]])\n",
    "assert_equal(grads[\"db\"],0.001455578136784208)\n",
    "assert_equal(cost,5.801545319394553)\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "During the training phase, after each forward step the parameters are updated according to the gradient. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(w, b, X, Y, num_iterations, learning_rate):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    :param w: weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    :param b: bias, a scalar\n",
    "    :param X: data of shape (num_px * num_px * 3, number of examples)\n",
    "    :param Y: target vector  of shape (1, number of examples)\n",
    "    :param num_iterations:  number of iterations of the optimization loop\n",
    "    :param learning_rate:  learning rate of the gradient descent update rule\n",
    "    \n",
    "    returns:\n",
    "    params: dictionary containing the weights w and bias b\n",
    "    grads: dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs: list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Compute gradient  and cost\n",
    "        grads, cost = forward(w, b, X, Y)\n",
    " \n",
    "        # Get derivatives for the parameters\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Update the parameters\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Store the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print (cost)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "    params = {\"w\": w,\"b\": b}\n",
    "    grads = {\"dw\": dw,\"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.801545319394553\n",
      "Cost after iteration 0: 5.801545\n",
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = train(w, b, X, Y, num_iterations= 100, learning_rate = 0.009)\n",
    "assert_almost_equal(costs[0],5.801545319394553)\n",
    "assert_allclose(params[\"w\"],[[0.19033591],[0.12259159]])\n",
    "assert_almost_equal(params[\"b\"],1.9253598300845747)\n",
    "assert_allclose(grads[\"dw\"],[[0.67752042], [1.41625495]])\n",
    "assert_almost_equal(grads[\"db\"],0.21919450454067652)\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class prediction on the dataset\n",
    "\n",
    "Now that you have trained the parameters w and b, you can use them  to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
    "\n",
    "1. Compute the activation $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "2. Convert the activation into classes according to a threshold : class 0 if activation <= 0.5 and class 1 if activation > 0.5\n",
    "3. stores the predictions into a vector `Y_prediction`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def decide(v):\n",
    "    #YOUR CODE HERE\n",
    "    if v> 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "vdecide = np.vectorize(decide)\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    \n",
    "    :params w: weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    :params b: bias, a scalar\n",
    "    :params X: data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    :returns: Y_prediction: a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of the target\n",
    "    A = sigmoid(w.T.dot(X)+b) #YOUR CODE HERE\n",
    "    \n",
    "    # Convert the probability into class using the decide function\n",
    "    print (A.shape)\n",
    "    #for i in range(A.shape[1]):\n",
    "    Y_prediction = vdecide(A) #YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "Y_prediction = predict(w, b, X)\n",
    "assert(Y_prediction.shape == (1, X.shape[1]))\n",
    "assert_allclose(Y_prediction,[[1,1,0]])\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full story\n",
    "\n",
    "A complete experiment is composed of both the training and evaluation phases. Write the function  `train_test()` :\n",
    "* train the network\n",
    "* predict on the test set\n",
    "* predict on the train set \n",
    "* compute the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X_train, Y_train, X_test, Y_test, num_iterations = 200, learning_rate = 0.5):\n",
    "    \"\"\"\n",
    "    Train and evaluate a logistic regression model\n",
    "    \n",
    "    Arguments:\n",
    "    :params X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    :params Y_train: training set tagets represented by a numpy array (vector) of shape (1, m_train)\n",
    "    :params X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    :params Y_test: test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    :params num_iterations:  number of training iterations (hyperparameter)\n",
    "    :params learning_rate: learning rate (hyperparameter)\n",
    "    \n",
    "    :returns: d: dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    dim = X_train.shape[0]\n",
    "    w, b = zero_initialize(dim)\n",
    "\n",
    "    # Train with gradient descent \n",
    "    parameters, grads, costs = train(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    \n",
    "    # Get trained parameters\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on binary MNIST \n",
    "The MNIST dataset is a standard dataset to test classification algorithms. It is composed of isolated digits on 28x28 pixels gray scale images. \n",
    "\n",
    "<img src=\"https://kermorvant.github.io/ml/dataiku/images/MnistExamples.png\" style=\"width:650px\" >\n",
    "\n",
    "\n",
    "A version of the MNIST dataset is available here : http://data.teklia.com/csexed/MNIST_all_features.csv.gz\n",
    "In this version, the images have been reduced to 8x8 pixels.\n",
    "\n",
    "Execute the following cells to get the data and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11272, 257)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the dataset\n",
    "#url=\"http://data.teklia.com/csexed/MNIST_all_features.csv.gz\"\n",
    "df = pd.read_csv(\"../data/MNIST_all_features16.csv.gz\")\n",
    "#df = pd.read_csv(url)\n",
    "# Select only 2 classes\n",
    "df_binary = df[(df['class']==8) | (df['class']==5)]\n",
    "print (df_binary.shape)\n",
    "# Select only a few examples\n",
    "df_train = df_binary[:8000]\n",
    "df_test = df_binary[8000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-57f1b82b362b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_set_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Select only features (drop the class), transpose and normalize in 0..1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_set_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtest_set_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"train_y:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_set_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"test_y:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_set_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"train_x:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"test_x:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_set_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "# Convert the target to numpy and convert to 0/1\n",
    "train_set_y = np.reshape(np.array(df_train['class'].replace(8,0).replace(5,1)),(1,-1))\n",
    "test_set_y = np.reshape(np.array(df_test['class'].replace(8,0).replace(5,1)),(1,-1))\n",
    "# Select only features (drop the class), transpose and normalize in 0..1\n",
    "train_set_x = df_train.drop('class',axis=1).transpose().divide(255.).as_matrix()\n",
    "test_set_x = df_test.drop('class',axis=1).transpose().divide(255.).as_matrix()\n",
    "print (\"train_y:\",train_set_y.shape,\"test_y:\",test_set_y.shape,\"train_x:\",train_set_x.shape,\"test_x:\",test_set_x.shape)\n",
    "print ()\n",
    "print ()\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation of logistic regression to classify 2 classes of digits from MNIST. In the next cell, you can display the images and you prediction results. Change the number of iteration, of samples, the classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = train_test(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 10000, learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Example of a picture that was wrongly classified.\n",
    "for index in range (20):\n",
    "    \n",
    "    print (\"index %i target = %s predicted = %f\" %(index,str(test_set_y[0,index]),d[\"Y_prediction_test\"][0,index]))\n",
    "    \n",
    "index = 14\n",
    "plt.imshow(test_set_x[:,index].reshape((8, 8, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
