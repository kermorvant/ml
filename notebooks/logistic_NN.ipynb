{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "In this notebook, you will program a complete classifier based on logistic regression seen as a neural network. The different steps are quite similar across the  all the kinds of neural networks but with different complexity. We will start with the simpliest one.\n",
    "\n",
    "## Logistic regresssion as a neural network\n",
    "\n",
    "The logistic regression classifier can be seen as a simple neural network : \n",
    "\n",
    "<img src=\"https://kermorvant.github.io/ml/images/logistic_NN.png\" style=\"width:650px\" >\n",
    "\n",
    "The weights are represented by the matrix W, the intercept by the bias b and the activation function is a sigmoid.\n",
    "\n",
    "### Basic functions\n",
    "\n",
    "First, we need to define 2 basic functions\n",
    "* the sigmoid activation functions. \n",
    "* the initialization function for the W and b parameters. \n",
    "\n",
    "In this notebook, the skeleton of the functions are given, you have to fill the spaces where there is a comment #YOUR CODE HERE. To check that your code is correct, a test is provided after each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the sigmoid function for activation\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    \n",
    "    :param z: a scalar or numpy array \n",
    "    :returns: sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-z)) #YOUR CODE HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "# Check that your code is correct on one example\n",
    "test_result = sigmoid(np.array([0,2]))\n",
    "assert_allclose(test_result,[0.5,0.880797])\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_initialize(dim):\n",
    "    \"\"\"\n",
    "    Initialize w and b to 0\n",
    "\n",
    "    :param dim: size of the vector w    \n",
    "    :returs: w vector of zeros of size (dim, 1) and b = 0\n",
    "\n",
    "    \"\"\"\n",
    "    w = np.zeros((dim,1)) #YOUR CODE HERE\n",
    "    b = 0 #YOUR CODE HERE\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "# Check that your code is correct on one example\n",
    "dim = 2\n",
    "w, b = zero_initialize(dim)\n",
    "assert(w.shape == (dim, 1))\n",
    "assert(isinstance(b, float) or isinstance(b, int))\n",
    "assert (b==0)\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward step\n",
    "\n",
    "The forward step consist in computing the prediction of the network. This step is used both during the training and , when the training is done, when the netwok is used to make predictions. In the next function, we will compute both the output of the network and the gradient to update the parameters.\n",
    "\n",
    "The activation is \n",
    "\\begin{equation*}\n",
    "A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\n",
    "\\end{equation*}\n",
    "The loss function on a sample i is : \n",
    "\\begin{equation*}\n",
    "L(\\hat a^{(i)}, y^{(i)}) = - y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "\\end{equation*}\n",
    "\n",
    "The cost computed on m samples is \n",
    "\\begin{equation*}\n",
    "J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "\\end{equation*}\n",
    "\n",
    "The gradient for each parameter is : \n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Compute the forward propagation and compute cost and gradient\n",
    "\n",
    "    \n",
    "    :param w: weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    :param b: bias, a scalar\n",
    "    :param X: data of size (num_px * num_px * 3, number of examples)\n",
    "    :param Y: the target vector of size (1, number of examples)\n",
    "\n",
    "    :returns:\n",
    "    cost: negative log-likelihood cost for logistic regression\n",
    "    dw: gradient of the loss with respect to w, thus same shape as w\n",
    "    db: gradient of the loss with respect to b, thus same shape as b\n",
    " \n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    #print(m)\n",
    "    # Compute activation\n",
    "    A = sigmoid((w.T).dot(X)+b) #YOUR CODE HERE\n",
    "    #print (A)\n",
    "    # Compute the cost\n",
    "    c1 = np.multiply(Y,np.log(A))+ np.multiply((1-Y),(np.log(1-A)))\n",
    "    #print (c1)\n",
    "    cost = -1.0/m*np.sum(c1) #YOUR CODE HERE\n",
    "    #print (cost)\n",
    "    # Compute derivative for gradient computation\n",
    "    dw = 1/m*X.dot((A-Y).T) #YOUR CODE HERE\n",
    "    db = 1/m*np.sum(A-Y) #YOUR CODE HERE\n",
    "\n",
    "    grads = {\"dw\": dw,\"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "# Check that your code is correct on one example\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = forward(w, b, X, Y)\n",
    "assert(grads[\"dw\"].shape == w.shape)\n",
    "assert(grads[\"db\"].dtype == float)\n",
    "cost = np.squeeze(cost)\n",
    "assert(cost.shape == ())\n",
    "assert_allclose(grads[\"dw\"],[[0.99845601],[2.39507239]])\n",
    "assert_equal(grads[\"db\"],0.001455578136784208)\n",
    "assert_equal(cost,5.801545319394553)\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "During the training phase, after each forward step the parameters are updated according to the gradient. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(w, b, X, Y, num_iterations, learning_rate):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    :param w: weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    :param b: bias, a scalar\n",
    "    :param X: data of shape (num_px * num_px * 3, number of examples)\n",
    "    :param Y: target vector  of shape (1, number of examples)\n",
    "    :param num_iterations:  number of iterations of the optimization loop\n",
    "    :param learning_rate:  learning rate of the gradient descent update rule\n",
    "    \n",
    "    returns:\n",
    "    params: dictionary containing the weights w and bias b\n",
    "    grads: dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs: list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Compute gradient  and cost\n",
    "        grads, cost = forward(w, b, X, Y)\n",
    " \n",
    "        # Get derivatives for the parameters\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Update the parameters\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Store the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print (cost)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "    params = {\"w\": w,\"b\": b}\n",
    "    grads = {\"dw\": dw,\"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.801545319394553\n",
      "Cost after iteration 0: 5.801545\n",
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = train(w, b, X, Y, num_iterations= 100, learning_rate = 0.009)\n",
    "assert_almost_equal(costs[0],5.801545319394553)\n",
    "assert_allclose(params[\"w\"],[[0.19033591],[0.12259159]])\n",
    "assert_almost_equal(params[\"b\"],1.9253598300845747)\n",
    "assert_allclose(grads[\"dw\"],[[0.67752042], [1.41625495]])\n",
    "assert_almost_equal(grads[\"db\"],0.21919450454067652)\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class prediction on the dataset\n",
    "\n",
    "Now that you have trained the parameters w and b, you can use them  to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
    "\n",
    "1. Compute the activation $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "2. Convert the activation into classes according to a threshold : class 0 if activation <= 0.5 and class 1 if activation > 0.5\n",
    "3. stores the predictions into a vector `Y_prediction`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def decide(v):\n",
    "    #YOUR CODE HERE\n",
    "    if v> 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "vdecide = np.vectorize(decide)\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    \n",
    "    :params w: weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    :params b: bias, a scalar\n",
    "    :params X: data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    :returns: Y_prediction: a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of the target\n",
    "    A = sigmoid(w.T.dot(X)+b) #YOUR CODE HERE\n",
    "    \n",
    "    # Convert the probability into class using the decide function\n",
    "    print (A.shape)\n",
    "    #for i in range(A.shape[1]):\n",
    "    Y_prediction = vdecide(A) #YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "all is correct !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "Y_prediction = predict(w, b, X)\n",
    "assert(Y_prediction.shape == (1, X.shape[1]))\n",
    "assert_allclose(Y_prediction,[[1,1,0]])\n",
    "print (\"all is correct !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full story\n",
    "\n",
    "A complete experiment is composed of both the training and evaluation phases. Write the function  `train_test()` :\n",
    "* train the network\n",
    "* predict on the test set\n",
    "* predict on the train set \n",
    "* compute the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(X_train, Y_train, X_test, Y_test, num_iterations = 200, learning_rate = 0.5):\n",
    "    \"\"\"\n",
    "    Train and evaluate a logistic regression model\n",
    "    \n",
    "    Arguments:\n",
    "    :params X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    :params Y_train: training set tagets represented by a numpy array (vector) of shape (1, m_train)\n",
    "    :params X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    :params Y_test: test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    :params num_iterations:  number of training iterations (hyperparameter)\n",
    "    :params learning_rate: learning rate (hyperparameter)\n",
    "    \n",
    "    :returns: d: dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    dim = X_train.shape[0]\n",
    "    w, b = zero_initialize(dim)\n",
    "\n",
    "    # Train with gradient descent \n",
    "    parameters, grads, costs = train(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    \n",
    "    # Get trained parameters\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on binary MNIST \n",
    "The MNIST dataset is a standard dataset to test classification algorithms. It is composed of isolated digits on 28x28 pixels gray scale images. \n",
    "\n",
    "<img src=\"https://kermorvant.github.io/ml/dataiku/images/MnistExamples.png\" style=\"width:650px\" >\n",
    "\n",
    "\n",
    "A version of the MNIST dataset is available here : http://data.teklia.com/csexed/MNIST_all_features.csv.gz\n",
    "In this version, the images have been reduced to 8x8 pixels.\n",
    "\n",
    "Execute the following cells to get the data and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11272, 65)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the dataset\n",
    "#url=\"http://data.teklia.com/csexed/MNIST_all_features.csv.gz\"\n",
    "df = pd.read_csv(\"../data/MNIST_all_features.csv\")\n",
    "#df = pd.read_csv(url)\n",
    "# Select only 2 classes\n",
    "df_binary = df[(df['class']==8) | (df['class']==5)]\n",
    "print (df_binary.shape)\n",
    "# Select only a few examples\n",
    "df_train = df_binary[:8000]\n",
    "df_test = df_binary[8000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y: (1, 8000) test_y: (1, 2000) train_x: (64, 8000) test_x: (64, 2000)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the target to numpy and convert to 0/1\n",
    "train_set_y = np.reshape(np.array(df_train['class'].replace(8,0).replace(5,1)),(1,-1))\n",
    "test_set_y = np.reshape(np.array(df_test['class'].replace(8,0).replace(5,1)),(1,-1))\n",
    "# Select only features (drop the class), transpose and normalize in 0..1\n",
    "train_set_x = df_train.drop('class',axis=1).transpose().divide(255.).as_matrix()\n",
    "test_set_x = df_test.drop('class',axis=1).transpose().divide(255.).as_matrix()\n",
    "print (\"train_y:\",train_set_y.shape,\"test_y:\",test_set_y.shape,\"train_x:\",train_set_x.shape,\"test_x:\",test_set_x.shape)\n",
    "print ()\n",
    "print ()\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation of logistic regression to classify 2 classes of digits from MNIST. In the next cell, you can display the images and you prediction results. Change the number of iteration, of samples, the classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454\n",
      "Cost after iteration 0: 0.693147\n",
      "0.3119067930912264\n",
      "Cost after iteration 100: 0.311907\n",
      "0.26695793029142684\n",
      "Cost after iteration 200: 0.266958\n",
      "0.24829016881050814\n",
      "Cost after iteration 300: 0.248290\n",
      "0.2377104966753284\n",
      "Cost after iteration 400: 0.237710\n",
      "0.23079416767692454\n",
      "Cost after iteration 500: 0.230794\n",
      "0.22588476994608656\n",
      "Cost after iteration 600: 0.225885\n",
      "0.2222072962765388\n",
      "Cost after iteration 700: 0.222207\n",
      "0.21934567115667175\n",
      "Cost after iteration 800: 0.219346\n",
      "0.2170546143540981\n",
      "Cost after iteration 900: 0.217055\n",
      "0.21517935581539047\n",
      "Cost after iteration 1000: 0.215179\n",
      "0.21361703428206652\n",
      "Cost after iteration 1100: 0.213617\n",
      "0.21229642160082574\n",
      "Cost after iteration 1200: 0.212296\n",
      "0.21116652028226038\n",
      "Cost after iteration 1300: 0.211167\n",
      "0.2101897922138036\n",
      "Cost after iteration 1400: 0.210190\n",
      "0.20933795406550962\n",
      "Cost after iteration 1500: 0.209338\n",
      "0.20858926704734002\n",
      "Cost after iteration 1600: 0.208589\n",
      "0.20792673335183104\n",
      "Cost after iteration 1700: 0.207927\n",
      "0.20733686243030325\n",
      "Cost after iteration 1800: 0.207337\n",
      "0.2068088064783268\n",
      "Cost after iteration 1900: 0.206809\n",
      "0.20633374161469897\n",
      "Cost after iteration 2000: 0.206334\n",
      "0.20590441647005878\n",
      "Cost after iteration 2100: 0.205904\n",
      "0.20551481727674245\n",
      "Cost after iteration 2200: 0.205515\n",
      "0.20515991558450877\n",
      "Cost after iteration 2300: 0.205160\n",
      "0.2048354755897836\n",
      "Cost after iteration 2400: 0.204835\n",
      "0.20453790514988623\n",
      "Cost after iteration 2500: 0.204538\n",
      "0.20426413926713724\n",
      "Cost after iteration 2600: 0.204264\n",
      "0.20401154802214078\n",
      "Cost after iteration 2700: 0.204012\n",
      "0.20377786313720186\n",
      "Cost after iteration 2800: 0.203778\n",
      "0.2035611188919897\n",
      "Cost after iteration 2900: 0.203561\n",
      "0.20335960420789875\n",
      "Cost after iteration 3000: 0.203360\n",
      "0.2031718235050073\n",
      "Cost after iteration 3100: 0.203172\n",
      "0.20299646450917816\n",
      "Cost after iteration 3200: 0.202996\n",
      "0.20283237160956302\n",
      "Cost after iteration 3300: 0.202832\n",
      "0.2026785236816238\n",
      "Cost after iteration 3400: 0.202679\n",
      "0.2025340155276619\n",
      "Cost after iteration 3500: 0.202534\n",
      "0.20239804226673475\n",
      "Cost after iteration 3600: 0.202398\n",
      "0.20226988614365773\n",
      "Cost after iteration 3700: 0.202270\n",
      "0.20214890533325786\n",
      "Cost after iteration 3800: 0.202149\n",
      "0.20203452439892566\n",
      "Cost after iteration 3900: 0.202035\n",
      "0.20192622612951808\n",
      "Cost after iteration 4000: 0.201926\n",
      "0.20182354452998613\n",
      "Cost after iteration 4100: 0.201824\n",
      "0.2017260587818993\n",
      "Cost after iteration 4200: 0.201726\n",
      "0.20163338802265812\n",
      "Cost after iteration 4300: 0.201633\n",
      "0.2015451868184224\n",
      "Cost after iteration 4400: 0.201545\n",
      "0.20146114122700334\n",
      "Cost after iteration 4500: 0.201461\n",
      "0.2013809653642124\n",
      "Cost after iteration 4600: 0.201381\n",
      "0.20130439840125305\n",
      "Cost after iteration 4700: 0.201304\n",
      "0.2012312019323003\n",
      "Cost after iteration 4800: 0.201231\n",
      "0.20116115766094816\n",
      "Cost after iteration 4900: 0.201161\n",
      "0.2010940653620914\n",
      "Cost after iteration 5000: 0.201094\n",
      "0.20102974108236682\n",
      "Cost after iteration 5100: 0.201030\n",
      "0.20096801554774585\n",
      "Cost after iteration 5200: 0.200968\n",
      "0.2009087327514511\n",
      "Cost after iteration 5300: 0.200909\n",
      "0.20085174869921502\n",
      "Cost after iteration 5400: 0.200852\n",
      "0.20079693029213774\n",
      "Cost after iteration 5500: 0.200797\n",
      "0.20074415433014725\n",
      "Cost after iteration 5600: 0.200744\n",
      "0.20069330662138132\n",
      "Cost after iteration 5700: 0.200693\n",
      "0.20064428118479352\n",
      "Cost after iteration 5800: 0.200644\n",
      "0.20059697953496528\n",
      "Cost after iteration 5900: 0.200597\n",
      "0.20055131003954818\n",
      "Cost after iteration 6000: 0.200551\n",
      "0.20050718734099654\n",
      "Cost after iteration 6100: 0.200507\n",
      "0.20046453183530646\n",
      "Cost after iteration 6200: 0.200465\n",
      "0.20042326920139614\n",
      "Cost after iteration 6300: 0.200423\n",
      "0.20038332997554464\n",
      "Cost after iteration 6400: 0.200383\n",
      "0.20034464916599246\n",
      "Cost after iteration 6500: 0.200345\n",
      "0.200307165903395\n",
      "Cost after iteration 6600: 0.200307\n",
      "0.20027082312333228\n",
      "Cost after iteration 6700: 0.200271\n",
      "0.2002355672775285\n",
      "Cost after iteration 6800: 0.200236\n",
      "0.20020134807081783\n",
      "Cost after iteration 6900: 0.200201\n",
      "0.20016811822123773\n",
      "Cost after iteration 7000: 0.200168\n",
      "0.20013583324092477\n",
      "Cost after iteration 7100: 0.200136\n",
      "0.20010445123574955\n",
      "Cost after iteration 7200: 0.200104\n",
      "0.20007393272185453\n",
      "Cost after iteration 7300: 0.200074\n",
      "0.20004424045746158\n",
      "Cost after iteration 7400: 0.200044\n",
      "0.20001533928848877\n",
      "Cost after iteration 7500: 0.200015\n",
      "0.1999871960066768\n",
      "Cost after iteration 7600: 0.199987\n",
      "0.19995977921905772\n",
      "Cost after iteration 7700: 0.199960\n",
      "0.19993305922772464\n",
      "Cost after iteration 7800: 0.199933\n",
      "0.19990700791896582\n",
      "Cost after iteration 7900: 0.199907\n",
      "0.1998815986609235\n",
      "Cost after iteration 8000: 0.199882\n",
      "0.19985680620902244\n",
      "Cost after iteration 8100: 0.199857\n",
      "0.1998326066184878\n",
      "Cost after iteration 8200: 0.199833\n",
      "0.19980897716333929\n",
      "Cost after iteration 8300: 0.199809\n",
      "0.1997858962613101\n",
      "Cost after iteration 8400: 0.199786\n",
      "0.19976334340418808\n",
      "Cost after iteration 8500: 0.199763\n",
      "0.19974129909313065\n",
      "Cost after iteration 8600: 0.199741\n",
      "0.1997197447785406\n",
      "Cost after iteration 8700: 0.199720\n",
      "0.1996986628041342\n",
      "Cost after iteration 8800: 0.199699\n",
      "0.1996780363548631\n",
      "Cost after iteration 8900: 0.199678\n",
      "0.19965784940838477\n",
      "Cost after iteration 9000: 0.199658\n",
      "0.19963808668980174\n",
      "Cost after iteration 9100: 0.199638\n",
      "0.19961873362941704\n",
      "Cost after iteration 9200: 0.199619\n",
      "0.19959977632327394\n",
      "Cost after iteration 9300: 0.199600\n",
      "0.19958120149626893\n",
      "Cost after iteration 9400: 0.199581\n",
      "0.19956299646764433\n",
      "Cost after iteration 9500: 0.199563\n",
      "0.19954514911868498\n",
      "Cost after iteration 9600: 0.199545\n",
      "0.19952764786245736\n",
      "Cost after iteration 9700: 0.199528\n",
      "0.19951048161544205\n",
      "Cost after iteration 9800: 0.199510\n",
      "0.19949363977092463\n",
      "Cost after iteration 9900: 0.199494\n",
      "(1, 2000)\n",
      "(1, 8000)\n",
      "train accuracy: 92.15 %\n",
      "test accuracy: 92.45 %\n"
     ]
    }
   ],
   "source": [
    "d = train_test(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 10000, learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 target = 0 predicted = 0.000000\n",
      "index 1 target = 1 predicted = 1.000000\n",
      "index 2 target = 1 predicted = 1.000000\n",
      "index 3 target = 1 predicted = 1.000000\n",
      "index 4 target = 0 predicted = 1.000000\n",
      "index 5 target = 0 predicted = 0.000000\n",
      "index 6 target = 1 predicted = 1.000000\n",
      "index 7 target = 1 predicted = 1.000000\n",
      "index 8 target = 0 predicted = 0.000000\n",
      "index 9 target = 1 predicted = 1.000000\n",
      "index 10 target = 0 predicted = 0.000000\n",
      "index 11 target = 1 predicted = 1.000000\n",
      "index 12 target = 1 predicted = 1.000000\n",
      "index 13 target = 0 predicted = 0.000000\n",
      "index 14 target = 0 predicted = 0.000000\n",
      "index 15 target = 1 predicted = 1.000000\n",
      "index 16 target = 0 predicted = 0.000000\n",
      "index 17 target = 1 predicted = 1.000000\n",
      "index 18 target = 1 predicted = 1.000000\n",
      "index 19 target = 1 predicted = 1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x118ce34a8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS40ZuNSYgUuNGbjUmIFLjRm41Nj/Af8BcDQ/isYZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ad9da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Example of a picture that was wrongly classified.\n",
    "for index in range (20):\n",
    "    \n",
    "    print (\"index %i target = %s predicted = %f\" %(index,str(test_set_y[0,index]),d[\"Y_prediction_test\"][0,index]))\n",
    "    \n",
    "index = 14\n",
    "plt.imshow(test_set_x[:,index].reshape((8, 8, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
