{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on the MNIST dataset\n",
    "\n",
    "The goal of this notebook is to train  several classifiers on the classical MNIST database. This database is very popular in the machine learning community as a first test for new algorithms. This dataset is quite simple and artificial : having good results on MNIST does not mean that your algorithm is good, but having bad results surely means that you have to improve your algorithm. \n",
    "\n",
    "\n",
    "<img src=\"https://kermorvant.github.io/csexed-ml/dataiku/images/MnistExamples.png\">\n",
    "\n",
    "\n",
    "\n",
    "You can find reference results on the MNIST dataset [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "### Libraries \n",
    "\n",
    "We will use the following libraries : \n",
    "* sklearn (scikit-learn) for machine learning algorithms\n",
    "* pandas for manipulating data\n",
    "* PIL for image processing\n",
    "* seaborn for graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Display the graphs in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "The first step is to extract features from the images to convert the image into a feature vectors. All the images  have the same size, 32x32 pixels. We have reduced them to 16x16 pixels and use the 256 pixels values vector as the features. This data is available here : https://kermorvant.github.io/csexed-ml/data/MNIST_all_features16.csv.gz\n",
    "\n",
    "\n",
    "The features for all the images is read from the file `MNIST_all_features16.csv.gz`. The features are stored in a matrix `X` and the target class in a vector `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...    247  248  249  250  251  252  253  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...      0    0    0    0    0    0    0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...      0    0    0    0    0    0    0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...      0    0    0    0    0    0    0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...      0    0    0    0    0    0    0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...      0    0    0    0    0    0    0   \n",
       "\n",
       "   254  255  class  \n",
       "0    0    0      4  \n",
       "1    0    0      5  \n",
       "2    0    0      7  \n",
       "3    0    0      8  \n",
       "4    0    0      0  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "READ_N_SAMPLES = 10000\n",
    "IMG_SIZE = 16\n",
    "all_df = pd.read_csv(\"../data/MNIST_all_features16.csv.gz\" ,nrows=READ_N_SAMPLES)\n",
    "\n",
    "# define the features and the targets\n",
    "X = all_df.iloc[:,:IMG_SIZE*IMG_SIZE]\n",
    "y = all_df['class']\n",
    "all_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all the features are equal to 0 but it is not the case: since the digits are centered, the first and last rows of pixels are white (=0). The `describe` allows to check the distribution of the features values : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>...</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.03870</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.06020</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>1.774400</td>\n",
       "      <td>3.795700</td>\n",
       "      <td>4.48780</td>\n",
       "      <td>3.578700</td>\n",
       "      <td>1.975800</td>\n",
       "      <td>0.799900</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.02010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.509489</td>\n",
       "      <td>2.29463</td>\n",
       "      <td>4.261334</td>\n",
       "      <td>5.294822</td>\n",
       "      <td>3.654368</td>\n",
       "      <td>2.57096</td>\n",
       "      <td>2.268511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.364046</td>\n",
       "      <td>7.515562</td>\n",
       "      <td>15.653383</td>\n",
       "      <td>23.696783</td>\n",
       "      <td>25.41829</td>\n",
       "      <td>22.960225</td>\n",
       "      <td>17.488768</td>\n",
       "      <td>10.743007</td>\n",
       "      <td>5.294778</td>\n",
       "      <td>1.22509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.904918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>201.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>147.00000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>80.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.00000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2        3        4        5        6        7  \\\n",
       "count  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0   \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "             8        9       10       11       12       13       14       15  \\\n",
       "count  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0   \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "            16       17       18       19       20            21           22  \\\n",
       "count  10000.0  10000.0  10000.0  10000.0  10000.0  10000.000000  10000.00000   \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.023900      0.03870   \n",
       "std        0.0      0.0      0.0      0.0      0.0      1.509489      2.29463   \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.000000      0.00000   \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.000000      0.00000   \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.000000      0.00000   \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.000000      0.00000   \n",
       "max        0.0      0.0      0.0      0.0      0.0    112.000000    201.00000   \n",
       "\n",
       "                 23            24            25           26            27  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.00000  10000.000000   \n",
       "mean       0.149500      0.206300      0.106900      0.06020      0.046600   \n",
       "std        4.261334      5.294822      3.654368      2.57096      2.268511   \n",
       "min        0.000000      0.000000      0.000000      0.00000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.00000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.00000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.00000      0.000000   \n",
       "max      175.000000    195.000000    177.000000    147.00000    157.000000   \n",
       "\n",
       "            28       29       30       31       32       33            34  \\\n",
       "count  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.000000   \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.003400   \n",
       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.294274   \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.000000   \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.000000   \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.000000   \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.000000   \n",
       "max        0.0      0.0      0.0      0.0      0.0      0.0     29.000000   \n",
       "\n",
       "           ...             222      223      224      225      226  \\\n",
       "count      ...       10000.000  10000.0  10000.0  10000.0  10000.0   \n",
       "mean       ...           0.008      0.0      0.0      0.0      0.0   \n",
       "std        ...           0.800      0.0      0.0      0.0      0.0   \n",
       "min        ...           0.000      0.0      0.0      0.0      0.0   \n",
       "25%        ...           0.000      0.0      0.0      0.0      0.0   \n",
       "50%        ...           0.000      0.0      0.0      0.0      0.0   \n",
       "75%        ...           0.000      0.0      0.0      0.0      0.0   \n",
       "max        ...          80.000      0.0      0.0      0.0      0.0   \n",
       "\n",
       "                227           228           229           230          231  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.00000   \n",
       "mean       0.027300      0.424000      1.774400      3.795700      4.48780   \n",
       "std        2.364046      7.515562     15.653383     23.696783     25.41829   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "max      233.000000    255.000000    255.000000    255.000000    255.00000   \n",
       "\n",
       "                232           233           234           235          236  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.00000   \n",
       "mean       3.578700      1.975800      0.799900      0.234800      0.02010   \n",
       "std       22.960225     17.488768     10.743007      5.294778      1.22509   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "max      255.000000    255.000000    255.000000    196.000000     85.00000   \n",
       "\n",
       "           237      238      239      240      241      242      243      244  \\\n",
       "count  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0   \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "           245      246      247      248      249      250      251      252  \\\n",
       "count  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0  10000.0   \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "           253      254      255         class  \n",
       "count  10000.0  10000.0  10000.0  10000.000000  \n",
       "mean       0.0      0.0      0.0      4.452100  \n",
       "std        0.0      0.0      0.0      2.904918  \n",
       "min        0.0      0.0      0.0      0.000000  \n",
       "25%        0.0      0.0      0.0      2.000000  \n",
       "50%        0.0      0.0      0.0      4.000000  \n",
       "75%        0.0      0.0      0.0      7.000000  \n",
       "max        0.0      0.0      0.0      9.000000  \n",
       "\n",
       "[8 rows x 257 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 70)\n",
    "all_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we plot the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x113424208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113303f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot class distriubtion\n",
    "sns.countplot(data=all_df,y='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the images after feature sub-resolution and check that our dataset is correct, i.e that the labels correspond to the image : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAFgCAYAAACmDI9oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYHWWZ/vH7JhFCQFljZA+BEJCIEVtgXBiUTTCAoLIM\nCCoYRJERF4I6QADHAfmBoigYlF0QcBkShk0QBRxUOoJsImIIgglJI4sBQUh8fn/UiVNJ1emuPlud\nU/l+rquvdD+pU/XUuft0v6e63ipHhAAAAAAkViq7AQAAAKCbMEAGAAAAUhggAwAAACkMkAEAAIAU\nBsgAAABACgNkAAAAIKVnB8i2z7N9QquXbbKn6bYv6/Rjex1ZVgt5VgdZVgt5VgdZtl9XDpBtz7X9\nou1Ftp+1/b+2P2b7n/1GxMci4tQi60sva3sn208Msf2LbH+pub1oL9tH2H7E9vO2b7C9ftk95SHL\nofVKlhJ5DsX2DrZ/Yvtp2wO2r7a9Xtl95SHLwdkeZztqr8ulH20fZDSKPIdme2fbD9n+m+1bbW9S\ndk95yHJonfi92ZUD5Jq9IuLVkjaRdJqkaZK+W25L3cH2TpK+LGkfSWtLelTSFWX2NASyrKMHs5TI\nczBrSZohaZyS52eRpAvLbGgIZDm0NSNi9dpHoQFJicizDtvrSvqRpBOU/Kztl3RlqU0Njizr6NTv\nzW4eIEuSIuK5iJgp6QBJh9meJGXf4dg+zvZ82/Nq7yzC9ubpZW2vJul6SeunjggM612H7bNtP277\nr7Zn237HcouMsn1l7Z3fb2y/MfXY9W3/sHZk6VHbxzT4tEyRdHVEPBARL0s6VdKOtjdrcH0dQZa5\nejJLiTzrPCfXR8TVEfHXiPibpHMkva2RdXUSWVYLeebaT9IDtdfnS5KmS3qj7S0bXF9HkGWujvze\n7PoB8lIR8WtJT0haPgzZfrekT0vaRdLmknaqs44XJO0haV7qiMC8YbZyl6TJSt61XC7patujUv+/\nj6SrU///37Zf5eRPI7Mk/VbSBpJ2lvQp27vnbcT2vbb/bZA+nPP5pGHuSynIMrtIzuc9kaVEnkPY\nUdIDw9yP0pBlrsdsP2H7QidHIXsGeS5j69p60vv1x1q965FldpGcz1v6e7NnBsg185Q86cvbX9KF\ntXcTf1PyzrAtIuKyiPhLRCyOiDMlrSJpYmqR2RHxg4h4RdJZkkZJ2kHSWySNiYhTIuLliJgj6XxJ\nB9bZzjYRcXmdNm6QtL/tbWyvKulESSFpdEt2sjPIMlGFLCXyzLC9jZI8P9fcnnUcWSaeqq1vE0lv\nlvRqSd9rxf51GHkmVpf03HK155Tk2ivIMtGR35sjW7myDthA0tM59fWVnE+01OPtasD2ZyUdXttm\nSHqNpPRRhX9uOyL+4eRk+KXLrm/72dSyIyTdPtweIuJm2ydJ+mFt+19Tcq7joCfedxmyVGWylMhz\n+V42V/KnzH+PiIbXUxKyTNb7vP5vfxfYPlrSfNuvjohFw11ficgz8Xxtu2mvUfLztleQpTr3e7Nn\nBsi236Lkm+OOnP+eL2nD1NcbDbKqaKKHd0g6TsmfBh6ohf+Mlj3Uv1Fq+ZVqfc2TtFjSoxExodHt\np0XENyV9s7adLST9h6T7W7HudiPLZfVylhJ55vSyiaSbJZ0aEZe2Yp2dQpaDWrpPPfOXV/JcxgOS\nDkttZzVJm6lHToEiy2V14vdm17/Qbb/G9hRJ35d0WUTcl7PYVZI+bHsr26OVzFKtZ4GkdWyvMcSm\nR9gelfpYWcmfYhZLGpA00vaJyr4jfbPt/WyPlPQpSX+X9EtJv5a0yPY026vaHmF7Uu2bflhq/Uxy\nYmMls+bPjohnhruuTiLLrF7NUiLPPLY3kPRTSedExHnDfXxZyDLL9va2J9peyfY6kr4u6WcRsfyf\n6bsOeeb6saRJtt/n5JzZEyXdGxEPNbCujiHLrE793uzmAfIs24uUHK7/opJzWT6ct2BEXK/kh9et\nkh5REoaUBLP8sg8puRzIHCfXF6w3g/N4SS+mPn4q6UYl5748LOkxSS8p+6eMa5TMNn1G0gcl7RcR\nr0TEEiUzLycruSTJU5K+Iyn3m9T2A7YPrtPbKCUnvz+v5JvuTg3+gigbWVYnS4k8B8vzCEnjJU13\n6vq5dZbtBmRZP8vxtT4WKTky9XdJB9VZtluQZ508I2JA0vsk/WdtO9urzvmvXYIsS/696YiGj7Z3\nLdtbKfmBtkpELC67HzSOLKuFPKuDLKuFPKuDLFujm48gD4vtfW2vYnstSadLmsU3Rm8iy2ohz+og\ny2ohz+ogy9arzABZ0pGSFiq5ruESSUeV2w6aQJbVQp7VQZbVQp7VQZYtVslTLAAAAIBGVekIMgAA\nANC0pq6D7OT2hmcrudjzdyLitMGWX3fddWPcuHHNbBItMHv27KciYkwz6yDL7tCKLCXy7Ba8NquD\n12a1kGd1FM2y4QGy7RFKLtK8q5K7l9xle2ZEPFjvMePGjVN/f3+9/0aH2H6s2XWQZXdoRZYSeXYL\nXpvVwWuzWsizOopm2cwpFttJeiQi5kTEy0ouYr1PE+sDAAAAStfMAHkDLXuB6CdqtWXYnmq733b/\nwMBAE5tD2ciyWsizOsiyWsizWsizN7V9kl5EzIiIvojoGzOm6dN3UCKyrBbyrA6yrBbyrBby7E3N\nDJD/LGmj1Ncb1moAAABAz2pmgHyXpAm2N7W9spJ7ms9sTVsAAABAORq+ikVELLZ9tKQblVzm7YKI\neKBlnQEAAAAlaOo6yBFxnaTrWtQLAAAAUDrupAcAAACkNHUEGQDa4W9/+1um9uCD2XsQ9fX1daId\nAMAKhiPIAAAAQAoDZAAAACCFATIAAACQwgAZAAAASGGSHoCOuO66/CtCvuc972npdq655ppMbe+9\n927pNjA8EZGpnXHGGZnacccd14l20AY33nhjpnbLLbdkamuuuWam9oUvfKEtPQHN4AgyAAAAkMIA\nGQAAAEhhgAwAAACkMEAGAAAAUpikt5ypU6dmavfdd1+mts4662Rqa6+9dqZ2ySWXtKYxNOSHP/xh\npvb+97+/4fXlTTZCMRMnTsytz5w5M1Pba6+9Cq3Tdqa2zz77ZGovvPBCpjZ69OhC28DwfPSjH83U\nvvOd7xR67LRp01raC6/X9sh73TXji1/8YqZGduV55ZVXMrW81/XFF19caH033HBDprb77rsPv7EO\n4wgyAAAAkMIAGQAAAEhhgAwAAACkMEAGAAAAUpqapGd7rqRFkpZIWhwRfa1oCgAAAChLK65i8c6I\neKoF62mrww8/PFO74IILMrWDDjooU8ubqbnFFltkau9617sytbzZvszObY+LLrooU/vwhz+cqRV9\n/idNmpSp7bDDDpnaL3/5y0LrW9Ftttlmw6oXkZdl3mtu++23z9Tyrk6D5m2++eaFlsvL7o477ii0\n3PPPP5+p7bHHHoW2i+FZffXVCy2Xl9NNN92UqRW9esHrXve6TO3JJ58s9FgUd/XVV2dq+++/f6aW\nN17K+5278847Z2q9cMWKPJxiAQAAAKQ0O0AOSTfZnm07ewFhSban2u633T8wMNDk5lAmsqwW8qwO\nsqwW8qwW8uxNzQ6Q3x4R20raQ9InbO+4/AIRMSMi+iKib8yYMU1uDmUiy2ohz+ogy2ohz2ohz97U\n1AA5Iv5c+3ehpB9L2q4VTQEAAABlaXiSnu3VJK0UEYtqn+8m6ZSWddZieSeY/+IXv8jU3vrWtxZa\n36GHHpqp3XrrrZnaUUcdVWh9GJ7bb789U2tmQl6eBx54oOHHojwzZszI1PJuIY/2OO200wot18zt\ninlttl7ehCsp/zbt8+fPz9TyJr3mTc469thjM7WzzjorU2v17awhLVy4MFPLm5DXzO/NvMfee++9\nmdo222zT8DY6pZmrWIyV9OPaN/FISZdHRPaG2wAAAEAPaXiAHBFzJL2xhb0AAAAApeMybwAAAEAK\nA2QAAAAgpRV30usJkydPztTe9ra3tX273/rWt9q+jRXRjjtmriiYOxGgqLFjxxZajrsgdr+PfvSj\nmVreJL3FixdnaiNHrjA/Etvm2WefLbRc0dfShhtumKltvfXWDa8P+fImOdeTd5e7vNrjjz+eqeXl\nWdRf/vKX3Po666zT8DpXJK997WsLLbfWWmsVWi7vbrN5d8PshQl5eTiCDAAAAKQwQAYAAABSGCAD\nAAAAKQyQAQAAgJQVZkbK3Xff3fBjX3zxxUxt9OjRmdrzzz/f8DbQvEsvvTRTO+OMMxpe3+9+97tm\n2kGLPffcc5naGmus0fD6mJDXnK9//euFl21mAt0TTzyRqeXdZS3vZ/yb3vSmhrdbZcPJo5mfg0Un\n5OXdrS8Pk/Far5nXZt7r8Mknn2ymna7CEWQAAAAghQEyAAAAkMIAGQAAAEhhgAwAAACkMEulgLwJ\neXl34VtttdU60Q4kTZgwIVPLm5DX39+fqeXdce8jH/lIprbllls22B3yvPTSS7n1VVddtcOdJGbN\nmpWp7bXXXiV00puOOeaY3PrBBx/c4U4SV111VabGJL18CxYsKLxsJ34OnnnmmW3fBprzqle9KlM7\n//zzM7Wid6XtBRxBBgAAAFIYIAMAAAApDJABAACAFAbIAAAAQMqQk/RsXyBpiqSFETGpVltb0pWS\nxkmaK2n/iHimfW12Tt6dYfLccccdbe4Eg3n44YcbfmxfX1+m9otf/KKZdlDANddcU3jZKVOmZGpv\nfetbM7UvfOELDfez9957Z2pXXHFF7rIHHnhgw9tZ0bT6bmdF78w1ffr0lm63yi6++OKyW1jGSSed\nlKmNGzeu841AkrTJJptkank/A4844ohOtFOaIkeQL5L07uVqx0u6JSImSLql9jUAAADQ84YcIEfE\nbZKeXq68j6Slb0EvlvTeFvcFAAAAlKLRc5DHRsT82udPSqp74TvbU2332+4fGBhocHPoBmRZLeRZ\nHWRZLeRZLeTZm5qepBcRISkG+f8ZEdEXEX1jxoxpdnMoEVlWC3lWB1lWC3lWC3n2pkbvpLfA9noR\nMd/2epIWtrKpTjnnnHMKLffQQw+1uRO0S9HJVXkTwNBa+++/f249L6Nrr722UC3Pe97znkztlFNO\nydTy7rzGZLx8c+fOzdQ6NYlqvfXWK7TcKqus0uZOquOAAw7I1I4/vjNTifbYY49Cyz366KNt7gSS\ntP3222dqK62UPXZ66aWXdqKdrtLoEeSZkg6rfX6YpOLT0wEAAIAuNuQA2fYVku6UNNH2E7YPl3Sa\npF1t/0HSLrWvAQAAgJ435CkWEXFQnf/aucW9AAAAAKXjTnoAAABASqOT9Crhk5/8ZKa20047ZWoT\nJ07sQDdohyuvvDJTmzVrVgmdoN5dKpML4SzrhBNOyNRefvnlTC3vDlyjR48u1M+2225baDlIm266\nadktLOPBBx8su4WeljfB8uyzz85dtujdZZvBz+TOOOSQQzK1X//615la3s/kFRFHkAEAAIAUBsgA\nAABACgNkAAAAIIUBMgAAAJDCABkAAABIWWGuYrHvvvsWWu7WW29tcydol7zbCeeZMmVKmztBs049\n9dSyW0DKcGa1P/TQQ5na1772tUxt1KhRmdqIESMytTPPPLPwttG4Y445pnD95ptvztTybgWfd8vi\nQw89NFObPHlykRZR0Lnnnptb/973vpepccWK+jiCDAAAAKQwQAYAAABSGCADAAAAKQyQAQAAgJQV\nZpLeF7/4xUwtb7IAeteJJ55YqAagfbbccstM7bzzziuhE7TLLrvsUqiGchx11FHDqiMfR5ABAACA\nFAbIAAAAQAoDZAAAACBlyAGy7QtsL7R9f6o23fafbd9T+9izvW0CAAAAnVFkkt5Fks6RdMly9a9G\nxP9reUdt0tfXV6gGAACAFduQR5Aj4jZJT3egFwAAAKB0zZyDfLTte2unYKxVbyHbU2332+4fGBho\nYnMoG1lWC3lWB1lWC3lWC3n2pkYHyOdK2kzSZEnzJZ1Zb8GImBERfRHRN2bMmAY3h25AltVCntVB\nltVCntVCnr2poQFyRCyIiCUR8Q9J50varrVtAQAAAOVwRAy9kD1O0rURMan29XoRMb/2+bGSto+I\nAwusZ0DSY4Mssq6kp4Zuuyd0875sEhFNvY0tkKXU3c/BcHXrvjSdpcRrs4vw2hy+bt0XXpvD1837\nQp7D1637UijLIQfItq+QtJOSHV0g6aTa15MlhaS5ko5cOmBuhu3+iKjEpSWqtC+NqtJzUKV9aUSV\n9r9K+9KoKj0HVdqXRlRp/6u0L42q0nPQ6/sy5GXeIuKgnPJ329ALAAAAUDrupAcAAACkdNsAeUbZ\nDbRQlfalUVV6Dqq0L42o0v5XaV8aVaXnoEr70ogq7X+V9qVRVXoOenpfCk3SAwAAAFYU3XYEGQAA\nACgVA2QAAAAghQEyAAAAkMIAGQAAAEhhgAwAAACkMEAGAAAAUhggAwAAACkMkAEAAIAUBsgAAABA\nSs8OkG2fZ/uEVi/bZE/TbV/W6cf2OrKsFvKsDrKsFvKsDrJsv64cINuea/tF24tsP2v7f21/zPY/\n+42Ij0XEqUXWl17W9k62nxhi+xfZ/lJze9E+tg+2/Xzq42+2w/aby+5teWQ5NNujbX/L9lO2n7N9\nW9k91UOeg+O1WZ0s02yfWMtxl7J7qYc8B2f79bb7bT9T+7jZ9uvL7isPWRbXztdmVw6Qa/aKiFdL\n2kTSaZKmSfpuuS11h4j4XkSsvvRD0sclzZH0m5Jbq4csBzdD0tqStqr9e2y57QyJPOvgtVk9tjeT\n9AFJ88vupQDyrG+epPcr+Rm7rqSZkr5fakeDI8shtPu12c0DZElSRDwXETMlHSDpMNuTpOw7HNvH\n2Z5ve57tI2rvKDZPL2t7NUnXS1o/dYRn/eH0Y/ts24/b/qvt2bbfsdwio2xfWXvn9xvbb0w9dn3b\nP7Q9YPtR28c0+LQs7zBJl0REtGh9bUGWuT1sKWlvSVMjYiAilkTE7EbW1WnkWQivzd7P8ptKBicv\nN7mejiHP3Ofk2YiYW3stWtISSZs3sq5OIstBtfW12fUD5KUi4teSnpC0fBiy/W5Jn5a0i5Jv+J3q\nrOMFSXtImpc6yjNvmK3cJWmyknehl0u62vao1P/vI+nq1P//t+1XOfnTyCxJv5W0gaSdJX3K9u55\nG7F9r+1/G6oZ25tI2lHSJcPcj9KQ5TK2k/SYpJOdnGJxn+33DXM/SkWe+Xht9n6Wtj8g6e8Rcd0w\n++8K5Jm7zLOSXpL0DUlfHuZ+lIYsM//f9tdmzwyQa+YpedKXt7+kCyPigYj4m6Tp7WogIi6LiL9E\nxOKIOFPSKpImphaZHRE/iIhXJJ0laZSkHSS9RdKYiDglIl6OiDmSzpd0YJ3tbBMRlxdo6VBJt0fE\no83sVwnIMrGhpEmSnpO0vqSjJV1se6tW7GMHkWcWr80GdUOWtl+tZAD1763bs1KQ57LLrClpDSU/\na+9ueuc6iyzVudfmyHauvA02kPR0Tn19Sf2prx9vVwO2Pyvp8No2Q9JrlJzPlNl2RPzDycnwS5dd\nv/budakRkm5vsqVD1UPvglPIMvGipFckfSkiFkv6ue1bJe0m6XcNrK8s5JnFa7NBXZLldEmXRsTc\nBh7bTchzORHxgu3zJA3Y3ioiFjazvg4iy8R0deC12TMDZNtvUfLNcUfOf89XciRuqY0GWVXD5wLW\nzrU5TsmfBh6ohf+MkvOZMtuu/UlhQyXv+hZLejQiJjS6/Zx+3qbkG+8HrVpnJ5DlMu7NqXX1+arL\nI8/cfnhtNt5Dt2S5s6QNbX+89vUYSVfZPj0iTm/B+tuOPAe1kqTRSp6frh8gk+UyOvLa7PpTLGy/\nxvYUJbNNL4uI+3IWu0rSh21vZXu0pMGu97dA0jq21xhi0yNsj0p9rCzp1UpCHpA00vaJSt49pb3Z\n9n62R0r6lKS/S/qlpF9LWmR7mu1VbY+wPan2Td+owyT9MCIWNbGOjiHLXLdJ+pOkz9seWRtYvVPS\njQ2sq6PIc1C8Nns/y52VnP40ufYxT9KRSiYGdTXyzLK9q+031dbxGiV//n9GXf6XOrLM1ZHXZjcP\nkGfZXqTkcP0XlXwzfzhvwYi4XtLXJd0q6RElYUhJMMsv+5CkKyTNcXJ9wXozOI9X8ufvpR8/VTJo\nuUHSw0omVr2k7J8yrlEy2/QZSR+UtF9EvBIRSyRNURLmo5KekvQdJedCZdh+wPbBdXqTk5Pi95d0\ncb1lughZ1smydp7WPpL2VHIe8vmSDq3tW7ciT16blc+ydp7lk0s/lFz14JmIeL7OvnQD8qz/2lyz\ntg/PSfqjpM0kvTsiXqqzfNnIsuTXpqO7rz7UECcTnO6XtErtvE70KLKsFvKsDrKsFvKsDrJsjW4+\ngjwstve1vYrttSSdLmkW3xi9iSyrhTyrgyyrhTyrgyxbrzIDZCXnnyxU8qeTJZKOKrcdNIEsq4U8\nq4Msq4U8q4MsW6ySp1gAAAAAjarSEWQAAACgaU1dB9nJ7Q3PVnKx5+9ExGmDLb/uuuvGuHHjmtkk\nWmD27NlPRcSYZtZBlt2hFVlK5NkteG1WB6/NaiHP6iiaZcMDZNsjlFxzblcl9we/y/bMiHiw3mPG\njRun/v7+ev+NDrH9WLPrIMvu0IosJfLsFrw2q4PXZrWQZ3UUzbKZUyy2k/RIRMyJiJeVXMR6nybW\nBwAAAJSumQHyBlr2AtFP1GrLsD3Vdr/t/oGBgSY2h7KRZbWQZ3WQZbWQZ7WQZ29q+yS9iJgREX0R\n0TdmTNOn76BEZFkt5FkdZFkt5Fkt5Nmbmhkg/1nSRqmvN6zVAAAAgJ7VzAD5LkkTbG9qe2VJB0qa\n2Zq2AAAAgHI0fBWLiFhs+2hJNyq5zNsFEfFAyzoDAAAAStDUdZAj4jpJ17WoFwAAAKB03EkPAAAA\nSGnqCDJQZffee2+m9sgjj2Rq++23XyfaAQAAHcIRZAAAACCFATIAAACQwgAZAAAASGGADAAAAKQw\nSQ+QdPvtt2dqO+64Y6b2m9/8phPtAJVy+umnZ2rTpk0roRN00oUXXpipPfzww5naRhttlKl9/OMf\nb0tP6G4f+chHMrULLrighE44ggwAAAAsgwEyAAAAkMIAGQAAAEhhgAwAAACkrNCT9J5++ulM7ZBD\nDsnU/vGPf2RqM2bMyNQ23njj1jSGtrrssssytQ9+8IOZWt6d9N7whje0pSegyo4//vhMjUl6vetT\nn/pUpnb22We3dBsTJkzI1HbdddeWbgPlOfzww3PreRM7maQHAAAAdAEGyAAAAEAKA2QAAAAghQEy\nAAAAkNLUJD3bcyUtkrRE0uKI6GtFUwAAAEBZWnEVi3dGxFMtWE9b2S603B577JGpTZkyJVPbZJNN\nMrW3ve1tmdodd9xRaLtoj8985jOZ2llnnZWp5V3RZK211mpLT1jWT3/600xt5513LvTY8ePHZ2pz\n5swp9NjXve51mdr8+fMLPRb1nXvuuZla3lV/mvHggw9maq9//etbug3UN3r06ELLLVq0KFNbddVV\nM7WRI7NDkT/96U/DbwwtkTde+vznP5+pffnLXy60vuuvvz5Tq3dliogotM5O4BQLAAAAIKXZAXJI\nusn2bNtT8xawPdV2v+3+gYGBJjeHMpFltZBndZBltZBntZBnb2p2gPz2iNhW0h6SPmF7x+UXiIgZ\nEdEXEX1jxoxpcnMoE1lWC3lWB1lWC3lWC3n2pqYGyBHx59q/CyX9WNJ2rWgKAAAAKEvDk/RsryZp\npYhYVPt8N0mntKyzFps9e3amtu222za8vv333z9TGzt2bMPrQ/PGjRuXqT322GOZ2u9///tMLW9C\nXtGJnd00qaCb5b0GpeIT8vIUnZCX58knn8zUrrzyytxlDzjggIa3s6L51re+landd999Da8vb6Lz\nJZdckqm1eiIg6subnFV0wlbRn6v1bkWM1nrrW99aaLmi+ebZc889M7Ubbrih4fV1SjNXsRgr6ce1\nb/aRki6PiO7fYwAAAGAQDQ+QI2KOpDe2sBcAAACgdFzmDQAAAEhhgAwAAACktOJOej2hmQl5eX70\nox9lamussUZLt4H6pk2blqnlTcjLuzNa3qS6vIkje+21V6Y2a9asTO2ZZ57J7ZE78S3rxRdfzK3f\neOONmdpuu+3W0m3n5fvBD34wU2MyXvOef/75lq7vK1/5SqY2c+bMlm4DzTvqqKMytfPOO6/QY2+7\n7bZWt4McU6dmb1dx5513ZmrNTDzP+1m7yy67ZGq77757w9voFI4gAwAAACkMkAEAAIAUBsgAAABA\nCgNkAAAAIGWFmaTXankTEvImcKE5zz33XG49b+LO//zP/2RqP/nJTzK1Qw89NFPLm5TwjW98I1PL\nu6sXk/GKefvb397ydT7++OOZ2sYbb1zosXl3Y0Pz5s6d29L15U2+veyyyzK1vNf6xRdf3NJekMi7\ny90FF1xQ6LGHHHJIpvaOd7yj6Z7wf4rerbCerbbaKlNbsmRJppb383fllVfO1PJem72AI8gAAABA\nCgNkAAAAIIUBMgAAAJDCABkAAABIYZJeAeecc06h5aZMmdLmTlY8jz76aOFlBwYGMrUPfehDmVre\nhLy8SQTHHHNMoceiPEUn5JFb5/zsZz/L1IpOGlpzzTUztWeffTZT+5d/+ZdMLW/yF9rj2GOPzdSK\nTtL79re/3ep2sJx58+bl1vPu9jt69OhM7fvf/36mdtBBBxXadpV+1nIEGQAAAEhhgAwAAACkMEAG\nAAAAUhggAwAAAClDTtKzfYGkKZIWRsSkWm1tSVdKGidprqT9I+KZ9rVZrk9+8pOZ2u9///sSOlnx\nrLrqqoWXzZuQN378+Ext9dVXz9ReeOGFTK1Kkw2qYKONNiq0HLmV61//9V8ztWYyee9735upbbbZ\nZg2vD82bNGlSppaXcd7kzPe///2Z2nXXXdeaxiBJWm+99Zp6/NSpUzO1UaNGZWovvvhiU9vpdkWO\nIF8k6d3L1Y6XdEtETJB0S+1rAAAAoOcNOUCOiNskPb1ceR9JS29yf7Gk7Ft8AAAAoAc1eg7y2IiY\nX/v8SUnNgUKZAAAYrElEQVRj6y1oe6rtftv9edepRe8gy2ohz+ogy2ohz2ohz97U9CS9SE48qnuC\nWUTMiIi+iOgbM2ZMs5tDiciyWsizOsiyWsizWsizNzV6J70FtteLiPm215O0sJVNlWnmzJmFltti\niy3a3AkkaeLEiU09fs6cOZnaZz/72UztjDPOaGo7aFze5J6VVir23p0JedVy7bXXZmp9fX0ldLLi\nmTBhQm794IMPztROOOGETG2XXXYptJ3rr79+eI2hrTbddNNMbdGiRZnaivizttEjyDMlHVb7/DBJ\n17SmHQAAAKBcQw6QbV8h6U5JE20/YftwSadJ2tX2HyTtUvsaAAAA6HlDnmIREQfV+a+dW9wLAAAA\nUDrupAcAAACkNDpJr7L22WefTO2Pf/xjCZ1gMHkTBs4555xM7eijj+5EO2jCm9/85kLLrYiTRFY0\n8+fPz9S23377EjpZ8XzgAx/IrZ988smFakVxF9ry7LDDDpna3LlzM7VXXnmlA910P44gAwAAACkM\nkAEAAIAUBsgAAABACgNkAAAAIIUBMgAAAJDCVSwKGD9+fNktoACuWNHdHnnkkdz63XffnalxxYoV\n09ixYzO17bbbroROVjxf/vKXc+vHHXdcpnb66adnarYztbwrY2yxxRYNdIdWmDZtWqa29957Z2oj\nRozoRDtdjyPIAAAAQAoDZAAAACCFATIAAACQwgAZAAAASGGS3nJ23XXXTO3ZZ5/N1NZcc81OtANU\nxuabb55b33333TO1X/7yl5la3m1SUS15E4ZQrrzfdf/1X/9VQido1r777lt2Cz2FI8gAAABACgNk\nAAAAIIUBMgAAAJAy5ADZ9gW2F9q+P1WbbvvPtu+pfezZ3jYBAACAzigySe8iSedIumS5+lcj4v+1\nvKOS3XTTTWW3AKxQbrjhhrJbAABgGUMeQY6I2yQ93YFeAAAAgNI1cw7y0bbvrZ2CsVa9hWxPtd1v\nu39gYKCJzaFsZFkt5FkdZFkt5Fkt5NmbGh0gnytpM0mTJc2XdGa9BSNiRkT0RUTfmDFjGtwcugFZ\nVgt5VgdZVgt5Vgt59qaGBsgRsSAilkTEPySdL2m71rYFAAAAlMMRMfRC9jhJ10bEpNrX60XE/Nrn\nx0raPiIOLLCeAUmPDbLIupKeGrrtntDN+7JJRDT1NrZAllJ3PwfD1a370nSWEq/NLsJrc/i6dV94\nbQ5fN+8LeQ5ft+5LoSyHHCDbvkLSTkp2dIGkk2pfT5YUkuZKOnLpgLkZtvsjoq/Z9XSDKu1Lo6r0\nHFRpXxpRpf2v0r40qkrPQZX2pRFV2v8q7UujqvQc9Pq+DHmZt4g4KKf83Tb0AgAAAJSOO+kBAAAA\nKd02QJ5RdgMtVKV9aVSVnoMq7UsjqrT/VdqXRlXpOajSvjSiSvtfpX1pVJWeg57el0KT9AAAAIAV\nRbcdQQYAAABKxQAZAAAASGGADAAAAKQwQAYAAABSGCADAAAAKQyQAQAAgBQGyAAAAEAKA2QAAAAg\nhQEyAAAAkNKzA2Tb59k+odXLNtnTdNuXdfqxvY4sq4U8q4Msq4U8q4Ms268rB8i259p+0fYi28/a\n/l/bH7P9z34j4mMRcWqR9aWXtb2T7SeG2P5Ftr/U3F60j+0dbP/E9tO2B2xfbXu9svvKQ5ZDs72/\n7d/VnqMHbb+37J7qIc/B2V7Z9g9qz1PY3qnsnuohy6HZPsL2I7aft32D7fXL7qke8hya7dG2v2X7\nKdvP2b6t7J7ykOXgbI+r/Xx9PvXR8jcAXTlArtkrIl4taRNJp0maJum75bbUNdaSNEPSOCXPzyJJ\nF5bZ0BDIsg7bG0i6TNKnJb1G0uckXW77taU2NjjyHNwdkg6R9GTZjRRAlnXU3tx8WdI+ktaW9Kik\nK8rsqQDyHNwMJVluVfv32HLbGRRZDm3NiFi99lHozcJwdPMAWZIUEc9FxExJB0g6zPYkKfsOx/Zx\ntufbnld71x+2N08va3s1SddLWj/1rmNYRwRsn237cdt/tT3b9juWW2SU7Str7/x+Y/uNqceub/uH\ntaO+j9o+psHn5PqIuDoi/hoRf5N0jqS3NbKuTiLLXBtKeraWaUTE/0h6QdJmDa6vY8gz9zl5OSK+\nFhF3SFrSyDrKQJa5pki6OiIeiIiXJZ0qaUfbvDZ7ME/bW0raW9LUiBiIiCURMbuRdXUSWZan6wfI\nS0XEryU9IWn5MGT73UqOwO0iaXNJO9VZxwuS9pA0L/WuY94wW7lL0mQl7z4vl3S17VGp/99H0tWp\n//9v269y8qeRWZJ+K2kDSTtL+pTt3fM2Yvte2/9WsKcdJT0wzP0oDVkuo1/S72zvbXuEk9Mr/i7p\n3mHuS2nIszrIMrtIzueThrkvpSHPZWwn6TFJJzs5xeI+2+8b5n6UhixzPWb7CdsX2l53mPsxpJ4Z\nINfMU/KkL29/SRfW3un/TdL0djUQEZdFxF8iYnFEnClpFUkTU4vMjogfRMQrks6SNErSDpLeImlM\nRJxSO8o0R9L5kg6ss51tIuLyofqxvY2kE5X8ab6XkGXyf0skXaLkB8nfa/8eWftB1kvIszrIMnGD\npP1tb2N7VSU/Z0PS6JbsZOeQZ2JDJW9unpO0vqSjJV1se6tW7GOHkGXiqdr6NpH0ZkmvlvS9Vuxf\n2shWr7DNNpD0dE59fSVH4pZ6vF0N2P6spMNr2wwl542m37n8c9sR8Q8nJ8MvXXZ928+mlh0h6fYm\netlcyZ9L/j0iGl5PScgy6WEXSV9R8o7/N0pe7DNt7xER9wx3fSUiz+ogy2S9N9s+SdIPa9v/mpL5\nHoNOcOpC5Jl4UdIrkr4UEYsl/dz2rZJ2k/S7BtZXBrJM1vu8/m9/F9g+WtJ826+OiEXDXV89PTNA\ntv0WJd8cd+T893wl7w6X2miQVUUTPbxD0nFK/jTwQC38Z7Tsn+E2Si2/Uq2veZIWS3o0IiY0uv3l\netlE0s2STo2IS1uxzk4hy2VMlnRbRCx9sd9l+1dK/lTWEwNk8qwOslxWRHxT0jdr29lC0n9Iur8V\n6+4E8lxG3mlrDe9Xp5HloJbuU0vPiuj6Uyxsv8b2FEnfl3RZRNyXs9hVkj5seyvboyUNdrmPBZLW\nsb3GEJseYXtU6mNlJYfxF0sakDTS9olK3j2lvdn2frZHSvqUkj+b/1LSryUtsj3N9qq1800n1b7p\nh8XJlQ9+KumciDhvuI8vC1nmukvSO2xPliTbb1JyjlnXn4NMnvlsr5I6J2/lWo8e9EElI8usWj+T\nnNhYyRUQzo6IZ4a7rk4jz1y3SfqTpM/bHmn7bZLeKenGBtbVMWSZZXt72xNtr2R7HUlfl/SziHhu\nuOsaTDcPkGfZXqTkcP0XlZzL8uG8BSPieiVP0K2SHlEShpQEs/yyDym5VM8cJ9cXrDeD83glf5JZ\n+vFTJS+kGyQ9rORk/5eU/VPGNUpmmz4j6YOS9ouIV2rnmk5RcsTwUSXn0HxHUu43qe0HbB9cp7cj\nJI2XNN2p6wDWWbYbkGWdLCPi50rOF/tB7Tn6oaQvR8RNdfalG5Bn/demJP2+1tcGtb5eVHKuXDci\ny/pZjlIyJ+B5Jb/c79TgA49uQJ71f9a+omQC2Z5KzkM+X9KhtX3rRmRZ/7U5vtbHIiV/0fm7pIPq\nLNswR/TMXxgKc3LS/f2SVqmda4QeRZbVQp7VQZbVQp7VQZat0c1HkIfF9r61P22uJel0SbP4xuhN\nZFkt5FkdZFkt5FkdZNl6lRkgSzpS0kJJf1Rygf6jym0HTSDLaiHP6iDLaiHP6iDLFqvkKRYAAABA\no6p0BBkAAABoWlPXQXZye8OzlVzs+TsRcdpgy6+77roxbty4ZjaJFpg9e/ZTETGmmXWQZXdoRZYS\neXYLXpvVwWuzWsizOopm2fAA2fYIJRdQ31XJnYXusj0zIh6s95hx48apv7+/3n+jQ2w/1uw6yLI7\ntCJLiTy7Ba/N6uC1WS3kWR1Fs2zmFIvtJD0SEXMi4mUlF7Hep4n1AQAAAKVrZoC8gZa9QPQTtdoy\nbE+13W+7f2BgoInNoWxkWS3kWR1kWS3kWS3k2ZvaPkkvImZERF9E9I0Z0/TpOygRWVYLeVYHWVYL\neVYLefamZgbIf5a0UerrDWs1AAAAoGc1M0C+S9IE25vaXlnSgZJmtqYtAAAAoBwNX8UiIhbbPlrS\njUou83ZBRDzQss4AAACAEjR1HeSIuE7SdS3qBQAAACgdd9IDAAAAUpo6ggwAAFDU9OnTM7WTTz45\nU1u8eHGmNmLEiHa0BOTiCDIAAACQwgAZAAAASGGADAAAAKQwQAYAAABSVphJeo8//nimdvvtt2dq\nBx98cEu3O3Xq1Ezt29/+dku3geZdeeWVmdqvfvWrTG3rrbfO1A4//PC29ITWOfTQQzO1Sy65pIRO\ngBXHxhtvnKnl/S6OiE60g4JeeOGFTG211VbL1K67LnuV3zvvvDNTe/TRRzO1Sy+9NHfbtou02BEc\nQQYAAABSGCADAAAAKQyQAQAAgBQGyAAAAEDKCjNJb++9987U7rnnnkztoIMOytQuv/zyQtv4yU9+\nkqnttttumdonPvGJ3Mdvs802hbaD4nbddddM7eabb27pNpik112mTZuWqeVNCGGSXrk23XTTTG3u\n3LmFHsukru6TN7lq/PjxmRrZlWfhwoWZ2tixY0voRDrhhBNy6xMnTuxwJ/VxBBkAAABIYYAMAAAA\npDBABgAAAFIYIAMAAAApTU3Ssz1X0iJJSyQtjoi+VjQFAAAAlKUVV7F4Z0Q81YL1tNXdd9/d9m3k\nXTEhT72rYnAVi+ZMmDAhU3vkkUcytRNPPDFTe/DBBzO1H/zgB5na6quv3mB36JSvfOUrmdr06dM7\n38gK6qqrrsrUDjjggIbXt9JKxf7QmXcVhZ///Oe5y+64444N94P853rSpEmZ2n333deJdlDQ1ltv\nXcp2x40bl6l109Uq6uEUCwAAACCl2QFySLrJ9mzbU/MWsD3Vdr/t/oGBgSY3hzKRZbWQZ3WQZbWQ\nZ7WQZ29qdoD89ojYVtIekj5hO/N3q4iYERF9EdE3ZsyYJjeHMpFltZBndZBltZBntZBnb2pqgBwR\nf679u1DSjyVt14qmAAAAgLI0PEnP9mqSVoqIRbXPd5N0Sss660FFb2G85pprtrmTFVPehLw8J598\ncqaWN+kkz6JFi4bVE9qraOYnnXRSmzvBUkUn5J155pmZ2qc//elCj7322msLLTdnzpzcOpP0isv7\n2fi6170uU2NCXvc455xzcutPPVXsegp77713pnbNNdc01VMvauYqFmMl/bj24hkp6fKIuKElXQEA\nAAAlaXiAHBFzJL2xhb0AAAAApeMybwAAAEAKA2QAAAAgpRV30quUl19+OVNbeeWVCz226J30jj/+\n+GH1hGLy7uR0//33Z2pFJ+RFRNM9ob3y7p6YN8EErXfWWWcVXvbHP/5xpvbe97630GOffvrpTG2v\nvfYq9NgPfehDhZaD9NBDDxVedv78+YWW+8Y3vpGpHXPMMYW3szx+Jhczfvz4ph4/c+bMTK3o783f\n/va3mVqv3iWYI8gAAABACgNkAAAAIIUBMgAAAJDCABkAAABI6elJeq+88kqmdsop+Tfz+9KXvtTw\ndtZdd91M7ROf+EShxx555JENbxfDc88992RqI0cW+xZfsmRJq9tBSVbEOz6VIW9SbD377rtvGztJ\nnH/++W3fRpUVvYthPUUncd12222Z2rbbbpupfe5zn8vUpkyZkrvOondWXFHsueeeufW8SY55v/ve\n8IY3ZGq/+93vCm37jW/M3h7jpptuytSKXtSgTBxBBgAAAFIYIAMAAAApDJABAACAFAbIAAAAQEpP\nT9LLu8Pd1ltvnbts0Tvw/OUvf8nU8ibpnXzyyZla3t2dzjvvvELbRfPOPffchh/71a9+NVP7zGc+\n00w7aLGxY8dmaltssUUJnUCSdtttt9x63mS5O+64I1N78sknM7Wvf/3rmdrEiRML9XPEEUcUWg75\nVl999cLL5k3EyrszYt4dFIvK+71b9K62KG7EiBGZ2oMPPljosWeccUamdtxxx2VqeXc3ffHFFwtt\no0wcQQYAAABSGCADAAAAKQyQAQAAgBQGyAAAAEDKkJP0bF8gaYqkhRExqVZbW9KVksZJmitp/4h4\npn1tFp+AlTcZRJLuv//+TO2QQw7J1H77298Or7GUOXPmNPxYNO+Tn/xkpvaBD3wgU9tggw0ytc9+\n9rOZGpP0usvChQsztQULFpTQCQaTN1mu1RPo7rvvvpauD9JVV12VW8+7Q969996bqeX97nzhhRcy\ntRkzZmRqeb/f//CHP2RqRSfbozPy7naYN0nvpZde6kQ7LVfkCPJFkt69XO14SbdExARJt9S+BgAA\nAHrekAPkiLhN0tPLlfeRdHHt84slZa/vAgAAAPSgRs9BHhsR82ufPykpe4HSGttTbffb7h8YGGhw\nc+gGZFkt5FkdZFkt5Fkt5Nmbmp6kF8lJQXVPDIqIGRHRFxF9Y8aMaXZzKBFZVgt5VgdZVgt5Vgt5\n9qZG76S3wPZ6ETHf9nqSsrNnWqzoHXTWWmutpraTN1kr724x06dPz9Te9a53NbVtFJc36TJP3sST\nj3zkI61uBy32vve9r+wWUIK8idN5Jk2a1OZOsFTexLi8iXt5taJOPfXUTO0//uM/Gl5fVf3973/P\n1EaNGpWpTZ48Offxu+66a6a25ZZbZmrz5s3L1E444YQiLeZ6z3ve0/Bjy9ToEeSZkg6rfX6YpGta\n0w4AAABQriEHyLavkHSnpIm2n7B9uKTTJO1q+w+Sdql9DQAAAPS8IU+xiIiD6vzXzi3uBQAAACgd\nd9IDAAAAUhqdpNdxhx9+eKFap+RN0kPnjB49utByRSeOnHfeec20gxb70Y9+lKlx17zq+973vld2\nCyiAO9qVY5VVVim03D333DOseit997vfzdR6dWI8R5ABAACAFAbIAAAAQAoDZAAAACCFATIAAACQ\nwgAZAAAASOmZq1gAaePHj8/ULr/88kztV7/6Vab2n//5n5naaqut1prG0BIHHZS9/PprX/vaEjpB\nuyxevLjQcrNmzWpzJ0DvGs4VRV5++eVM7eGHH87U7rzzzkwt72fy6quvXnjbvYgjyAAAAEAKA2QA\nAAAghQEyAAAAkMIAGQAAAEhhkh4qI28SQV4N3S9vwiWqZeTI7K8fbmEMtM/KK6+cqU2aNKlQbUXE\nEWQAAAAghQEyAAAAkMIAGQAAAEgZcoBs+wLbC23fn6pNt/1n2/fUPvZsb5sAAABAZxQ5gnyRpHfn\n1L8aEZNrH9e1ti0AAACgHEMOkCPiNklPd6AXAAAAoHTNnIN8tO17a6dgrFVvIdtTbffb7h8YGGhi\ncygbWVYLeVYHWVYLeVYLefamRgfI50raTNJkSfMlnVlvwYiYERF9EdE3ZsyYBjeHbkCW1UKe1UGW\n1UKe1UKevamhAXJELIiIJRHxD0nnS9qutW0BAAAA5XCROxfZHifp2oiYVPt6vYiYX/v8WEnbR8SB\nBdYzIOmxQRZZV9JTQ7fdE7p5XzaJiKbexhbIUuru52C4unVfms5S4rXZRXhtDl+37guvzeHr5n0h\nz+Hr1n0plOWQA2TbV0jaScmOLpB0Uu3ryZJC0lxJRy4dMDfDdn9E9DW7nm5QpX1pVJWegyrtSyOq\ntP9V2pdGVek5qNK+NKJK+1+lfWlUlZ6DXt+XkUMtEBEH5ZS/24ZeAAAAgNJxJz0AAAAgpdsGyDPK\nbqCFqrQvjarSc1ClfWlElfa/SvvSqCo9B1Xal0ZUaf+rtC+NqtJz0NP7UmiSHgAAALCi6LYjyAAA\nAECpGCADAAAAKV0zQLb9btu/t/2I7ePL7mc4arfbXmj7/lRtbds/sf2H2r91b8ddNWRZLeRZHWRZ\nLeRZHWTZfbpigGx7hKRvStpD0uslHWT79eV2NSwXSXr3crXjJd0SERMk3VL7uvLIslrIszrIslrI\nszrIsjt1xQBZya2qH4mIORHxsqTvS9qn5J4Ki4jbJD29XHkfSRfXPr9Y0ns72lR5yLJayLM6yLJa\nyLM6yLILdcsAeQNJj6e+fqJW62VjU3cXfFLS2DKb6SCyrBbyrA6yrBbyrA6y7ELdMkCutEiupcf1\n9CqALKuFPKuDLKuFPKujV7PslgHynyVtlPp6w1qtly2wvZ4k1f5dWHI/nUKW1UKe1UGW1UKe1UGW\nXahbBsh3SZpge1PbK0s6UNLMkntq1kxJh9U+P0zSNSX20klkWS3kWR1kWS3kWR1k2Y0iois+JO0p\n6WFJf5T0xbL7GWbvV0iaL+kVJecOHS5pHSUzN/8g6WZJa5fdJ1mSJXmu2HmSZbU+yLM6H2TZfR/c\nahoAAABI6ZZTLAAAAICuwAAZAAAASGGADAAAAKQwQAYAAABSGCADAAAAKQyQAQAAgBQGyAAAAEDK\n/wdyH0xdWZJauAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10554f390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "number_plot_images = 18\n",
    "random_indices = random.sample(range(READ_N_SAMPLES), number_plot_images)\n",
    "\n",
    "fig, axes = plt.subplots(3,6, \n",
    "                        figsize=(10,5),\n",
    "                        sharex=True, sharey=True,\n",
    "                        subplot_kw=dict( aspect='equal')) \n",
    "\n",
    "for i in range(number_plot_images):\n",
    "    \n",
    "    subplot_row = i//6 \n",
    "    subplot_col = i%6  \n",
    "    ax = axes[subplot_row, subplot_col]\n",
    "    # plot image on subplot\n",
    "    plottable_image = all_df.iloc[random_indices[i],:IMG_SIZE*IMG_SIZE].values.reshape((IMG_SIZE,IMG_SIZE))\n",
    "    ax.imshow(plottable_image, cmap='gray_r')\n",
    "    \n",
    "    ax.set_title('Digit Label: {}'.format(all_df['class'][random_indices[i]]))\n",
    "    ax.set_xbound([0,IMG_SIZE])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/dev/test split\n",
    "\n",
    "When training a classifier, the data **must** be separated into different sets : at least one training set and one test set. The split must be random and uniform, which means that the class distribution must be identical in the training and test sets.\n",
    "\n",
    "**Question:**\n",
    "> * Use [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create `X_train/y_train` and `X_test/y_test`. Use 80% of the data for training and 20% for testing.\n",
    "> * Train a [k-nearest neighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classififier with k=1\n",
    "> * Test the k-NN with k= 1 on both the training and the test set. Print the score produced by [`clf.score()`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score)\n",
    "\n",
    "When evaluating a classifier, it is important to report the error rate both on the training and the classification set. These values are needed to understand what is wrong with the classifier and how to improve it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "\n",
    "test_percent = None # YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(None, None, test_size=None, random_state=32)# YOUR CODE HERE\n",
    "\n",
    "# create a kNN classifier with a given k\n",
    "k = None # YOUR CODE HERE\n",
    "clf = neighbors.KNeighborsClassifier(None,n_jobs=-1)# YOUR CODE HERE\n",
    "\n",
    "# Train the classifier on training set\n",
    "clf.fit(None,None)# YOUR CODE HERE\n",
    "\n",
    "# Predict and evaluate on train set\n",
    "print ('Train accuracy:',clf.score(None, None))# YOUR CODE HERE\n",
    "# Predict and evaluate on test set\n",
    "print ('Test accuracy:',clf.score(None,None))# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization\n",
    "\n",
    "\n",
    "The main parameter of the kNN algorithm is the number of neighbors (k). The best value for this parameter depends on the classification task and has to be found by trying different values and selecting the one with the best accuracy. However, this search for the best value **must not** be done on the set used to evaluate the classifier (the test set) but on a validation set. \n",
    "\n",
    "**Question** : \n",
    "\n",
    "\n",
    ">  * Create three sets : train set (60%), validation set (20%) and test set (20%), using twice `train_test_split`\n",
    ">  * Train a kNN classifier with different values of k in [1,3,5,7 9] and report the train/valid/test accuracy. \n",
    ">  * Select the is best value for k according to the accuracy on the dev set. Report the performance performance of the classifier on the test set for this value of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Create Train/dev/test sets\n",
    "# YOUR CODE HERE\n",
    "# Create validation set so that train = 60% , validation = 20% and test =  20%\n",
    "# Create X_train, X_dev, y_train, y_dev, X_test, y_test\n",
    "\n",
    "\n",
    "#  list of k values to test\n",
    "k_values = [None] # YOUR CODE HERE\n",
    "\n",
    "# store the score in a dataframe\n",
    "df_scores = pd.DataFrame(columns=['train','dev','test'],index=k_values)\n",
    "\n",
    "# iterate on diffÃ©rent values of k\n",
    "for k in None:# YOUR CODE HERE\n",
    "    print(\"k={}\".format(k))\n",
    "    \n",
    "    # create a kNN classifier with a given k\n",
    "    clf = neighbors.KNeighborsClassifier(None,n_jobs=-1) # YOUR CODE HERE\n",
    "    \n",
    "    # Train the classifier on training set\n",
    "    clf.fit(None,None) # YOUR CODE HERE\n",
    "    \n",
    "    # Compute the classification score on the different sets\n",
    "    for _name,_X,_y in [('train',X_train,y_train),('dev',X_dev,y_dev),('test',X_test,y_test)]:\n",
    "        df_scores.at[k,_name] = float(\"{:.3f}\".format(clf.score(None,None))) # YOUR CODE HERE\n",
    "        clear_output(wait=True)\n",
    "        print(df_scores)\n",
    "_g = df_scores.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Another simple yet very effective classifier is the logisitic regression. This classifier is a version of the linear regression model adapted to classification. \n",
    "\n",
    "**Question** : \n",
    "\n",
    "> Using the MNIST Data, train a [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier (`clf = linear_model.LogisticRegression()`) train set and  report the accuracy on dev and test sets. Compare to the best result of the kNN classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# Train a logisitic regression model \n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(None,None) # YOUR CODE HERE\n",
    "\n",
    "print (\"logistic regression accuracy on dev set:\",clf.score(None,None)) # YOUR CODE HERE\n",
    "print (\"logistic regression accuracy on test set:\",clf.score(None,None)) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the classifiers depends on the size of the training set : the more data, the better accuracy. Will study the impact of the size of the training set on kNN and Logisitic Regression.\n",
    "\n",
    "For this study, we first load 2000 training samples and we separate the data in train/test set with 80%/20% ratio. Then we train the classifier on an increasing training set corresponding to  `[1%,10%,30%,50%,70%,100%]` percents of the training set. We will always be tested on the same test set (the original one). The following Figure explains the different splits : \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://kermorvant.github.io/csexed-ml/images/learning_curve_sets.png\" width=\"300\" >\n",
    "</p>\n",
    "\n",
    "\n",
    "**Question** : \n",
    "\n",
    "\n",
    "> * Report the training and test set accuracies for the Logisitic Regresstion, 1NN and kNN (k being the best value for `k` you previously found) on an increasing training set of [1,10,30,50,70,100] percent of the initial training set.\n",
    "> * Plot the training curves on a plot similar to : \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://kermorvant.github.io/csexed-ml/images/training_curves.png\" width=\"300\" align=\"center\">\n",
    "</p>\n",
    "\n",
    "You can use [pandas plot function](https://pandas.pydata.org/pandas-docs/stable/visualization.html#basic-plotting-plot).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "nb_train = 2000\n",
    "print (nb_train)\n",
    "SIZES = [int(nb_train*x/100.) for x in [None]] # YOUR CODE HERE\n",
    "df_scores = pd.DataFrame(columns=['LogRegTrain','LogRegTest','1NNTrain','1NNTest','2NNTrain','2NNTest'],index=SIZES)\n",
    "\n",
    "# define the 3 classifiers\n",
    "clf1 = None # YOUR CODE HERE\n",
    "clf2 = None # YOUR CODE HERE\n",
    "clf3 = None # YOUR CODE HERE\n",
    "\n",
    "# loop on the training set size\n",
    "for sub_size in SIZES:\n",
    "    # get the index of the selected samples and extract the corresponding targets\n",
    "    X_sub = X_train.iloc[:sub_size]  \n",
    "    y_sub = y_train.iloc[:sub_size]\n",
    "    \n",
    "    # train the models\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # compute accuracy on train set\n",
    "    df_scores.at[sub_size,'LogRegTrain'] =  None # YOUR CODE HERE\n",
    "    # compute accuracy on test set\n",
    "    df_scores.at[sub_size,'LogRegTest'] =  None # YOUR CODE HERE\n",
    "    \n",
    "    # compute accuracy on train set\n",
    "    df_scores.at[sub_size,'1NNTrain'] =  None # YOUR CODE HERE\n",
    "    # compute accuracy on test set\n",
    "    df_scores.at[sub_size,'1NNTest'] =  None # YOUR CODE HERE\n",
    "    \n",
    "    # compute accuracy on train set\n",
    "    df_scores.at[sub_size,'2NNTrain'] =  None # YOUR CODE HERE\n",
    "    # compute accuracy on test set\n",
    "    df_scores.at[sub_size,'2NNTest'] =  None # YOUR CODE HERE\n",
    "    clear_output(wait=True)\n",
    "    print(df_scores)\n",
    "styles1 = ['bs--','bs-','r^--','r^-','go--','go-']\n",
    "df_scores.plot(style=styles1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "\n",
    "Scikit-learn has a special class for dealing with hyperparamter optimization :  [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "We can search for the best values of the hyperparameters  by defining a pipeline. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', neighbors.KNeighborsClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'clf__n_neighbors':(1,3)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the GridSearchCV object will be fitted on the train set with all the possible combinaison of parameter values and evaluated on a validation set with cross validation. The train/dev spit and the cross-validation is done automatically. Progress can be monitored with greater values of the parameter verbose.\n",
    "\n",
    "**Question** : \n",
    "\n",
    "\n",
    ">  *  reproduce the experiments on the parameter k in [1,3,5,7,9] from the previous question using GridSearchCV\n",
    ">  * add the exploration of different values of the feature dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected block\n",
    "\n",
    "    # Define the grid search to find the best parameters for both the feature extraction and the classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Split the dataframe with the file names in train/test\n",
    "    df_train_dev, df_test, y_train_dev, y_test = train_test_split(all_df, all_df['class'], test_size=0.2)\n",
    "    \n",
    "    print(\"Performing grid search\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    \n",
    "    # Run the grid search\n",
    "    grid_search.fit( None,None) # YOUR CODE HERE\n",
    "    all_score = pd.DataFrame(grid_search.cv_results_)\n",
    "    print (all_score[['params','mean_train_score','mean_test_score']])\n",
    "    \n",
    "    # Print all experiments results\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "    \n",
    "    # Print best experiment results\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    # predict on test with best parameters\n",
    "    y_pred = grid_search.predict(None) # YOUR CODE HERE\n",
    "    print(classification_report(None,None)) # YOUR CODE HERE\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "\n",
    "We will now optimize a Support Vector Machine, SVM (SÃ©parateurs Ã  Vaste Marge in French) classifier on the MNIST database. These classifiers usually give very good results if they are well tuned.\n",
    "\n",
    "All the SVM classifier share a common parameter `C`: it controls how many examples are allowed to be badly classified during the optimization. For small values of C, some training example are allowed to be misclassified if the margin  (the distance between the separating line and the support vector) is large. For large values of C, the algorithm tries to minimize the number of misclassified training example, even if it lead to a small margin. The impact of the value for C is shown on the following figure : \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://kermorvant.github.io/csexed-ml/images/svm_values_for_C.png\" width=\"400\" >\n",
    "</p>\n",
    " \n",
    "\n",
    "\n",
    "The RBF kernel (see the [scikit-learn kernels documentation](http://scikit-learn.org/stable/modules/svm.html#svm-kernels) has one more main parameter that must be optimized on the data :  `gamma`.\n",
    "\n",
    "`gamma` is a parameter controlling the *spread* of the RBF kernel : if `gamma` is small, the kernel takes into account many training samples and the decision boundary is smooth. When `gamma` is large, the kernel is focused on few training examples and the decision boundary is complex. The impact of `gamma` is illustrated on the following Figures : \n",
    "\n",
    "- `gamma` = 1\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://kermorvant.github.io/csexed-ml/images/svc_parameters_using_rbf_kernel_17_0.png\" width=\"300\" >\n",
    "</p>\n",
    "\n",
    "- `gamma` = 100\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://kermorvant.github.io/csexed-ml/images/svc_parameters_using_rbf_kernel_21_0.png\" width=\"300\" >\n",
    "</p>\n",
    "\n",
    "Moreover, for the RBF kernel, the data must be normalizedn you can use the [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to scale to [zero mean and unit variance](https://en.wikipedia.org/wiki/Feature_scaling#Standardization).  \n",
    "\n",
    "**Question** : \n",
    "> * add the StandardScaler as a pipeline step\n",
    "> * add the RBF kernel to the GridSearch [svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) \n",
    "> * add the optimization of  `gamma` and `C` with values :  `C in [0.1,1,5,10,50]` and `gamma in [0.0005,0.001,0.005,0.01,0.05]`. \n",
    "\n",
    "You can add a different kernel with its specific parameters this way : \n",
    "<pre>\n",
    "        {\n",
    "         'features__dim': (8,),\n",
    "         'clf__kernel': ['rbf'],\n",
    "         'clf__gamma': [1e-3, ],\n",
    "         'clf__C': [1,]\n",
    "        },\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "READ_N_SAMPLES = 10000\n",
    "IMG_SIZE = 16\n",
    "all_df = pd.read_csv(\"MNIST_all_features16.csv.gz\" ,nrows=READ_N_SAMPLES)\n",
    "all_df.head()\n",
    "# define the features and the targets\n",
    "X = all_df.iloc[:,:IMG_SIZE*IMG_SIZE]\n",
    "\n",
    "y = all_df['class']\n",
    "\n",
    "sns.countplot(data=all_df,y='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # YOUR CODE HERE\n",
    "    ('clf', svm.SVC())    \n",
    "])\n",
    "\n",
    "gamma_range = None # YOUR CODE HERE\n",
    "C_range = None # YOUR CODE HERE\n",
    "parameters = [\n",
    "        {\n",
    "        'clf__kernel': ['linear'],\n",
    "        'clf__C': C_range\n",
    "        }\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=2, cv=2)\n",
    "\n",
    "    X_train_dev, X_test, y_train_dev, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    print (X_train_dev.shape,X_test.shape,y_train_dev.shape,y_test.shape)\n",
    "    print(\"Performing grid search\")\n",
    "\n",
    "    grid_search.fit( None,  None) # YOUR CODE HERE\n",
    "    \n",
    "    # Print all experiments results\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "    \n",
    "    # Print best experiment results\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "    # predict on test with best parameters\n",
    "    y_pred = grid_search.predict(None)# YOUR CODE HERE\n",
    "    print(classification_report(None, None))# YOUR CODE HERE\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots are from https://github.com/ksopyla/svm_mnist_digit_classification \n",
    "from matplotlib.colors import Normalize\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "def plot_param_space_scores(scores, C_range, gamma_range):\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.jet,\n",
    "               norm=MidpointNormalize(vmin=0.5, midpoint=0.9))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "scores = grid_search.cv_results_['mean_test_score'].reshape(len(C_range),\n",
    "                                                     len(gamma_range))\n",
    "\n",
    "plot_param_space_scores(scores, C_range, gamma_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found the best classifier (SVM RBF) and the best hyperparameter values, we can train the final classifier on all the data and evaluate it.\n",
    "\n",
    "**Question** : \n",
    "> * train the best classifier with the best hyperparameter values on 80% of the data and evaluate on 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train on all the data\n",
    "READ_N_SAMPLES = 20000\n",
    "IMG_SIZE = 16\n",
    "all_df = pd.read_csv(\"MNIST_all_features16.csv.gz\" ,nrows=READ_N_SAMPLES)\n",
    "all_df.head()\n",
    "# define the features and the targets\n",
    "X = all_df.iloc[:,:IMG_SIZE*IMG_SIZE]\n",
    "y = all_df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(None, None, test_size=0.2)# YOUR CODE HERE\n",
    "classifier = svm.SVC(gamma=None,C=None,verbose=True) # YOUR CODE HERE\n",
    "classifier.fit( None,  None)# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = classifier.predict(X_test)\n",
    "classification_report(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
